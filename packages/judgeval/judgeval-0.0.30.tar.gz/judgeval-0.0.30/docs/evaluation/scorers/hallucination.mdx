---
title: Hallucination
description: ""
---

The `Hallucination` scorer is a default LLM judge scorer that measures how much the `actual_output` contains information that contradicts the `context`.

<Note>
**If you're building an app with a RAG pipeline, you should try the `Faithfulness` scorer instead.**
 
The `Hallucination` scorer is concerned with `context`, the ideal retrieved context, while `Faithfulness` is concerned with `retrieval_context`, the actual retrieved context. 
</Note>

## Required Fields

To run the `Hallucination` scorer, you must include the following fields in your `Example`:
- `input`
- `actual_output`
- `context`

## Scorer Breakdown

`Hallucination` scores are calculated by determining for each document in `context`, whether there are any contradictions to `actual_output`. 
The score is then calculated as:

$$
\text{Hallucination} = \frac{\text{Number of Contradicted Documents}}{\text{Total Number of Documents}}
$$

## Sample Implementation

```python hallucination.py
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import HallucinationScorer

client = JudgmentClient()
example = Example(
    input="What's your return policy for a pair of socks?",
    # Replace this with your LLM system's output
    actual_output="We offer a 30-day return policy for all items, including socks!",
    # Replace this with the contexts passed to your LLM as ground truth
    context=["**RETURN POLICY** all products returnable with no cost for 30-days after purchase (receipt required)."]
)
# supply your own threshold
scorer = HallucinationScorer(threshold=0.8)

results = client.run_evaluation(
    examples=[example],
    scorers=[scorer],
    model="gpt-4o",
)
print(results)
```