# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------

"""utility functions for XAI."""

import base64
import tempfile
from io import BytesIO

import colorcet as cc
import numpy as np
import torchvision.transforms as T
from azureml.automl.dnn.vision.classification.common.constants import ModelParameters
from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionDataException
from azureml.automl.dnn.vision.common.logging_utils import get_logger
from azureml.automl.dnn.vision.explainability.constants import (
    IntegratedGradientsDefaults,
    ExplainabilityLiterals,
    XAIVisualizationDefaults,
    XAIPredictionLiterals,
    ExplainabilityDefaults,
)
from captum.attr import visualization as viz
from matplotlib import pyplot as plt
from PIL import Image
from torch.utils.data import Dataset

logger = get_logger(__name__)


class XaiMultiLabelCustomDataset(Dataset):
    """This class replicates single images for explainability in a multilabel setting."""

    def __init__(self, annotations, image, probs_batch):
        """Initialize annotations, image, probs_batch.

        :param annotations: Image labels.
        :type annotations: torch.Tensor
        :param image: Single image.
        :type image: torch.Tensor
        :param probs_batch: Multilabel probabilities of each image in batch
        :type probs_batch: torch.Tensor
        """
        self.img_labels = annotations
        self.image = image
        self.probs_batch = probs_batch

    def __len__(self):
        """Return length of img_labels.

        :return: Length of img_labels
        :rtype: int
        """
        return len(self.img_labels)

    def __getitem__(self, idx):
        """Return image and corresponding label, probabilities.

        :param idx: Index.
        :type idx: int
        """
        return self.image, self.img_labels[idx], self.probs_batch[idx]


def base64_to_img(base64ImgString):
    """Convert base64 image string to bytes

    :param base64ImgString: base64 image string
    :type base64ImgString: str
    :return: bytes image
    :rtype: bytes
    """
    base64Img = base64ImgString.encode("utf-8")
    decoded_img = base64.b64decode(base64Img)
    return BytesIO(decoded_img).getvalue()


def img_to_base64(img_path):
    """Load and convert pillow image to base64-encoded image

    :param img_path: image path
    :type img_path: str
    :return: base64-encoded image
    :rtype: str
    """
    img = Image.open(img_path)
    imgio = BytesIO()
    img.save(imgio, img.format)
    img_str = base64.b64encode(imgio.getvalue())
    return img_str.decode("utf-8")


def save_captum_visualizations(
    xai_model_name, attributions, sample_image, pred_label_name, prediction_score
):
    """Generates visualizations for XAI

    :param xai_model_name: XAI method/model name
    :type xai_model_name: str
    :param attributions: attributions tensor generated by XAI model
    :type attributions: torch.Tensor
    :param sample_image: input image augmented
    :type sample_image: torch.Tensor
    :param pred_label_name: predicted label name
    :type pred_label_name: str
    :param prediction_score: prediction or confidence score
    :type prediction_score: float
    :return: figure/image in base64 string format
    :rtype: base64 string
    """

    logger.info("Generating XAI visualizations for {}.".format(xai_model_name))
    # input sample_image pixels are normalized during inference, for appropriate visualization,
    # we need to reverse the transformation
    mean = np.array(ModelParameters.DEFAULT_IMAGE_MEAN)
    std = np.array(ModelParameters.DEFAULT_IMAGE_STD)
    unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())
    unnormalized_input_tensor = unnormalize(sample_image)
    unnormalized_image = np.transpose(
        unnormalized_input_tensor.squeeze().cpu().detach().numpy(), (1, 2, 0)
    )

    # creating the 2x2 figure for visualizing explanations
    fig, ax = plt.subplots(2, 2, figsize=(12, 12), tight_layout=True)
    fig.patch.set_facecolor("white")
    # display the unnormalized input image at (0, 0) index in figure
    ax[0][0].imshow(unnormalized_image, cmap=cc.cm.gray, interpolation="lanczos")
    ax[0][0].set_title("Augmented Input Image")

    # display the unnormalized input image superimposed with attribution scores at (0, 1)
    # index in figure
    fig, _ = viz.visualize_image_attr(
        np.transpose(attributions.squeeze(0).cpu().detach().numpy(), (1, 2, 0)),
        unnormalized_image,
        method="heat_map",
        sign="absolute_value",
        show_colorbar=True,
        cmap=cc.cm.bgyw,
        title=xai_model_name + " heatmap",
        plt_fig_axis=(fig, ax[0][1]),
        use_pyplot=False,
    )

    # display the unnormalized input image superimposed with heatmap of attribution scores
    # at (1, 0) index in figure
    fig, _ = viz.visualize_image_attr(
        np.transpose(attributions.squeeze(0).cpu().detach().numpy(), (1, 2, 0)),
        unnormalized_image,
        method="blended_heat_map",
        sign="absolute_value",
        show_colorbar=True,
        cmap=cc.cm.bgyw,
        title=xai_model_name + " on image",
        plt_fig_axis=(fig, ax[1][0]),
        use_pyplot=False,
    )

    # display only the top 30% of pixels based on attribution scores in unnormalized input image
    # and blackout remaining pixels at (1, 1) index in figure
    attr = attributions.squeeze(0).cpu().detach().numpy()
    top_pixel_percent = 100 - XAIVisualizationDefaults.top_x_percent_attributions
    title_1_1 = "Top {}% attributions".format(
        str(XAIVisualizationDefaults.top_x_percent_attributions)
    )
    top_30_percent = attr > np.percentile(attr, top_pixel_percent)
    top_30_percent = np.transpose(top_30_percent, (1, 2, 0))

    unnormalized_image[~top_30_percent] = 0
    ax[1][1].imshow(unnormalized_image, cmap=cc.cm.gray, interpolation="lanczos")
    ax[1][1].set_title(title_1_1)

    fig.suptitle(
        "Predicted_label: "
        + str(pred_label_name)
        + ", Confidence_score: "
        + str(round(prediction_score, 4)),
        fontsize=XAIVisualizationDefaults.figure_title_fontsize,
    )
    plt.close()

    # save the figure, convert it to base64 and return
    with tempfile.NamedTemporaryFile(delete=True) as img_path_to_store:
        fig.savefig(
            img_path_to_store,
            bbox_inches="tight",
            dpi=XAIVisualizationDefaults.savefig_dpi
        )
        return img_to_base64(img_path_to_store)


def validate_xai_parameters(**kwargs):
    """
    Validate explainability input parameter values
    """

    n_steps = kwargs.get(
        ExplainabilityLiterals.N_STEPS, IntegratedGradientsDefaults.N_STEPS
    )
    if (
        not isinstance(n_steps, int)
        or n_steps < IntegratedGradientsDefaults.N_STEPS_MIN
    ):
        raise AutoMLVisionDataException(
            "Invalid value for {}".format(ExplainabilityLiterals.N_STEPS), has_pii=False
        )

    approximation_method = kwargs.get(
        ExplainabilityLiterals.APPROXIMATION_METHOD, IntegratedGradientsDefaults.METHOD
    )
    if approximation_method not in IntegratedGradientsDefaults.ALL_METHODS:
        raise AutoMLVisionDataException(
            "Invalid value for {}. Recommended methods are {}.".format(
                ExplainabilityLiterals.APPROXIMATION_METHOD,
                IntegratedGradientsDefaults.ALL_METHODS,
            ),
            has_pii=False,
        )

    visualizations = kwargs.get(
        XAIPredictionLiterals.VISUALIZATIONS_KEY_NAME,
        ExplainabilityDefaults.OUTPUT_VISUALIZATIONS,
    )
    if not isinstance(visualizations, bool):
        raise AutoMLVisionDataException(
            "Invalid boolean value for {}".format(
                XAIPredictionLiterals.VISUALIZATIONS_KEY_NAME
            ),
            has_pii=False,
        )

    attributions = kwargs.get(
        XAIPredictionLiterals.ATTRIBUTIONS_KEY_NAME,
        ExplainabilityDefaults.OUTPUT_ATTRIBUTIONS,
    )
    if not isinstance(attributions, bool):
        raise AutoMLVisionDataException(
            "Invalid boolean value for {}".format(
                XAIPredictionLiterals.ATTRIBUTIONS_KEY_NAME
            ),
            has_pii=False,
        )
