# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_leg_sbt.ipynb.

# %% auto 0
__all__ = ['execute_cmd_and_log', 'setup_logger', 'run_sbt_typing', 'parse_vcf', 'get_border_reads', 'get_positions_from_sam',
           'match_positions_to_border_reads', 'mompS2_mapping_stats', 'print_mompS2_fasta', 'parse_mompS2_allele_blast',
           'get_alleles_from_blast', 'run_sbt_blast', 'extract_blast_sbt', 'load_allele_comp_to_ST_dict',
           'find_best_ST_match_to_alleles', 'get_depth_from_sam', 'cli']

# %% ../nbs/02_leg_sbt.ipynb 3
# standard libs
import os
import re
import sys

# Common to template
# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`
import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/
import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml
import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/
from fastcore import (
    test,
)
from fastcore.script import (
    call_parse,
)  # for @call_parse, https://fastcore.fast.ai/script
import json  # for nicely printing json and yaml

# import functions from core module (optional, but most likely needed).
from legionella_sbt import (
    core,
)

# Project specific libraries

import pandas as pd
from pathlib import Path
from sindr.seq_tools import DnaSeq
import subprocess
import logging
from copy import deepcopy

# %% ../nbs/02_leg_sbt.ipynb 6
def execute_cmd_and_log(
    cmd, logger, log_stdout=True, log_stderr=True
) -> tuple[str, str]:
    logger.info(f"Running command: {cmd}")
    process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        shell=True,
        encoding="utf-8",
    )
    stdout, stderr = process.communicate()
    if log_stdout and stdout and stdout is not None:
        logger.info(f"Shell command STDOUT: {stdout}")
    if log_stderr and stderr and stderr is not None:
        logger.error(f"Shell command STDERR: {stderr}")
    return stdout, stderr


def setup_logger(log_file, log_level="INFO") -> logging.RootLogger:
    logger = logging.getLogger()
    logging.basicConfig(
        level=log_level,
        filename=str(log_file),
        encoding="utf-8",
        filemode="w",
        format="{asctime} - {levelname} - {message}",
        style="{",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    return logger


def run_sbt_typing(
    assembly_file: Path,
    r1_file: Path,
    r2_file: Path,
    mompS2_reference_fasta: Path,
    mompS2_allele_fasta: Path,
    sbt_allele_blast_db: Path,
    sbt_profiles_tsv: Path,
    output_dir: Path,
    output_prefix: str = "Legionella_SBT",
    nopath: bool = True,
    clean_files: bool = True,
) -> None:

    if not os.path.exists(output_dir):
        try:
            os.mkdir(output_dir)
        except FileNotFoundError as e:
            print(f"Failed to setup mompS output directory: {e}", file=sys.stderr)
            return None

    prefix = os.path.join(output_dir, output_prefix)
    sbt_blast_output = (
        f"{prefix}.sbt_blast.tsv"  # output from regular blastn against all sbt alleles
    )
    sam = f"{prefix}.sam"  # sam file produced from bwa mem mapping to mompS2
    bam = f"{prefix}.bam"  # bam file filtered on q20 and only including mapped reads from primary alignments
    sorted_bam = f"{prefix}.sorted.bam"  # sorted bam file with index (.bam.bai)
    sorted_sam = f"{prefix}.sorted.sam"  # sorted sam file for parsing
    vcf = f"{prefix}.vcf"  # variant calls
    mompS2_fasta_out = (
        f"{prefix}.mompS2.fasta"  # fasta file with determined mompS2 sequence
    )
    mompS2_allele_blast_out = f"{prefix}.mompS2.blast.tsv"  # blast output from blasting mompS2 sequence against known alleles
    mompS2_stats_file = (
        f"{prefix}.mompS2.stats.txt"  # file with statistics for mompS2 mapping
    )
    sbt_results_file = f"{prefix}.sbt.tsv"
    log_file = f"{prefix}.log"  # log file

    # Load SBT profiles
    gene_names, allele_comp_to_ST = load_allele_comp_to_ST_dict(sbt_profiles_tsv)

    logger = setup_logger(log_file=log_file)
    logger.info(f"RUNNING LEGIONELLA SBT")
    logger.info(f"Checking input files")
    print("Checking for input files and resources")
    all_input_files_found = True
    all_reference_files_found = True
    for f in [assembly_file, r1_file, r2_file]:
        if not Path(f).exists():
            all_input_files_found = False
            logger.critical(f"Input file missing at {f}, exiting")
            print(f"Input file missing at {f}, exiting", file=sys.stderr)

    for f in [
        mompS2_reference_fasta,
        mompS2_allele_fasta,
        sbt_allele_blast_db,
        sbt_profiles_tsv,
    ]:
        if not Path(f).exists():
            all_reference_files_found = False
            logger.critical(f"Resource data file missing at {f}, exiting")
            print(f"Resource data file missing at {f}, exiting", file=sys.stderr)

    if all_input_files_found and all_reference_files_found:
        print("All input files and resource files found")
        print("Running in-silico Legionella SBT")
        logger.info(f"All input files and reference data files found")
        logger.info(f"Using inputs:")
        logger.info(f"Assembly file: {assembly_file}")
        logger.info(f"Forward read file: {r1_file}")
        logger.info(f"Reverse read file: {r2_file}")
        logger.info(f"Using resources:")
        logger.info(f"mompS2 reference: {mompS2_reference_fasta}")
        logger.info(f"mompS2 alleles fasta: {mompS2_allele_fasta}")
        logger.info(f"SBT allele blast database: {sbt_allele_blast_db}")
        logger.info(f"SBT allele profiles: {sbt_profiles_tsv}")
        logger.info(f"Printing to output directory: {output_dir}")
        logger.info(
            f"------------------------------------------------------------------------------------------------------------------------"
        )
        logger.info(
            f"Identifying best matching alleles in assembly file {assembly_file}"
        )

        # Run blast against sbt allele db
        # cmd = f"blastn -query {assembly_file} -sbt {sbt_allele_blast_db} -out {sbt_blast_output} -qcov_hsp_perc 80 -perc_identity 95 -num_alignments 10000 -outfmt '6 qseqid sseqid pident length slen qstart qend sstart send sseq evalue bitscore'"
        print("Blasting to identify SBT alleles from assembly")
        cmd = f"blastn -query {sbt_allele_blast_db} -subject {assembly_file} -out {sbt_blast_output} -qcov_hsp_perc 60 -outfmt '6 qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore'"
        stdout, stderr = execute_cmd_and_log(cmd, logger)

        # Extract best matching alleles from blast output
        try:
            allele_dict, detailed_dict = extract_blast_sbt(sbt_blast_output)
        except:
            print(f"Failed to load SBT blast output from {sbt_blast_output}. Exiting.")
            logger.critical(
                f"Failed to load SBT blast output from {sbt_blast_output}. Exiting."
            )

            if nopath:
                sample_name = Path(assembly_file).name
            else:
                sample_name = assembly_file
            sbt_results_out = open(sbt_results_file, "w")
            sbt_results_out.write("sample\tST\t" + "\t".join(gene_names) + "\tnotes\n")
            sbt_results_out.write(
                sample_name
                + "\t"
                + "\t".join(["-"] * 8)
                + "\tNo matches found for SBT alleles\n"
            )
            sbt_results_out.close()
            return None

        logger.info("")
        logger.info(
            f"------------------------------------------------------------------------------------------------------------------------"
        )
        logger.info(f"STARTING MOMPS TYPING")
        print("Mapping reads to mompS2 reference")
        # Run bwa mem, mapping all reads to mompS2 reference
        cmd = f"bwa mem -v 1 -o {sam} {mompS2_reference_fasta} {r1_file} {r2_file} 2> /dev/null"
        if not os.path.exists(sam):
            stdout, stderr = execute_cmd_and_log(cmd=cmd, logger=logger)

        # Run samtools view to filter unmapped reads and convert to bam
        # -F 260 to only include mapped and exclude secondary alignments,
        # cmd = f"samtools view -q 20 -h -F 260 --input-fmt-option 'filter= [NM]/rlen < 0.06' -O BAM -o {bam} {sam}"    DO NOT  filter to only include reads with less than 6 mismatches
        # cmd = f"samtools view -q 20 -h -F 260 --input-fmt-option 'filter=[NM]<12' -O BAM -o {bam} {sam}"
        cmd = f"samtools view -q 20 -h -F 260 -O BAM -o {bam} {sam}"
        if not os.path.exists(bam):
            stdout, stderr = execute_cmd_and_log(cmd=cmd, logger=logger)

        # Sort and index bam with samtools
        cmd = f"samtools sort {bam} -o {sorted_bam}; samtools index -o {sorted_bam}.bai {sorted_bam}"
        if not os.path.exists(sorted_bam):
            stdout, stderr = execute_cmd_and_log(cmd=cmd, logger=logger)

        # Run samtools view to convert bam to sam
        cmd = f"samtools view -h -O SAM -o {sorted_sam} {sorted_bam}"  # -F 260 to only include mapped and exclude secondary alignments
        if not os.path.exists(sorted_sam):
            stdout, stderr = execute_cmd_and_log(cmd=cmd, logger=logger)

        # run bcftools call to generate vcf
        cmd = f"bcftools mpileup -f {mompS2_reference_fasta} {sorted_sam} | bcftools call -mv -Ov -o {vcf}"
        if not os.path.exists(vcf):
            stdout, stderr = execute_cmd_and_log(
                cmd=cmd, logger=logger, log_stdout=False, log_stderr=False
            )

        # get border read IDs (reads that span the mompS2 unique regions at the start and end of momps2 and are therefore unique to the mompS2 allel)
        stdout, stderr, border_readIDs, all_reads = get_border_reads(
            sam_file=sorted_sam
        )
        logger.info(stdout)

        # Get consensus mompS2 sequence along with list of positions with ambiguity
        seq_list, amb_positions, mompS_heterozygous = parse_vcf(
            reference_fasta=mompS2_reference_fasta, vcf_file=vcf
        )
        logger.info(
            f"{len(amb_positions)} ambiguous positions identified in mompS gene from {vcf}"
        )

        mompS2_stat_lines = []
        low_confidence_warnings = []

        (
            error_msg,
            low_cov_positions_in_ST_region,
            low_cov_positions_outside_ST_region,
        ) = get_depth_from_sam(sam_file=sorted_sam)

        if error_msg:
            logger.critical(f"Error parsing depths from {sorted_sam}.")
        else:
            if len(low_cov_positions_in_ST_region) > 0:
                logger.warning(
                    f"Low depth positions in allele defining region of mompS2 found in {sorted_sam}:"
                )
                low_confidence_warnings.append("Low coverate in allele defining region")
                for pos, depth in low_cov_positions_in_ST_region.items():
                    logger.warning(f"Low depth at position {pos}:  {depth}")
            if len(low_cov_positions_outside_ST_region) > 0:
                logger.warning(
                    f"Low depth positions outside allele defining region of mompS2 found in {sorted_sam}:"
                )
                low_confidence_warnings.append(
                    "Low coverate outside allele defining region"
                )
                for pos, depth in low_cov_positions_outside_ST_region.items():
                    logger.warning(f"Low depth at position {pos}:  {depth}")

        if mompS_heterozygous:
            print("Heterozygous mompS alleles identified. Identifying mompS2 copy.")
            # If the two mompS copies are heterozygous in allele defining region, run the process to identify which reads map exclusively to mompS2
            logger.info(
                f"mompS gene copies are heterozygous in allele defining regions"
            )
            logger.info(
                f"Ambiguous base calls found at positions: {','.join(str(item) for item in amb_positions)}"
            )

            # Get a dictionary with the base call from each read positions with ambiguity
            pos_read_call_dict, read_pos_call_dict = get_positions_from_sam(
                sam_file=sorted_sam, ambiguous_positions=amb_positions
            )
            logger.info(
                f"Base calls at ambiguous positions extracted from reads in {sorted_sam}"
            )

            # Use ambiguous positions to identify mompS2 specific alleles from border reads
            (
                mompS2_consensus_dict,
                all_calls,
                mompS2_calls,
                mompS2_border_calls,
                mompS2_readIDs,
            ) = match_positions_to_border_reads(
                pos_read_call_dict=pos_read_call_dict,
                read_pos_call_dict=read_pos_call_dict,
                border_readIDs=border_readIDs,
            )
            logger.info(
                f"{len(mompS2_readIDs)} mompS2-specific reads identified from ambiguous positions and {len(border_readIDs)} initial border reads"
            )

            (
                positions_to_ignore,
                consistent_mompS1_reads,
                consistent_mompS2_reads,
                inconsistent_mompS_reads,
                position_consistency_dict,
            ) = mompS2_mapping_stats(
                read_pos_call_dict,
                mompS2_consensus_dict,
                mompS2_readIDs,
                border_readIDs,
            )

            mompS2_stat_lines.append(f"Iteration 1\n")
            mompS2_stat_lines.append(
                f"Extracting mompS gene from all variable positions\n"
            )
            mompS2_stat_lines.append(
                f"------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
            )
            mompS2_stat_lines.append(f"mompS mapping details\n")
            mompS2_stat_lines.append(
                f"Number of reads consistent with mompS1 on all positions (more than one position): {len(consistent_mompS1_reads)}\n"
            )
            mompS2_stat_lines.append(
                f"Number of reads consistent with mompS2 on all positions (more than one position): {len(consistent_mompS2_reads)}\n"
            )
            mompS2_stat_lines.append(
                f"Inconsistent reads: {len(inconsistent_mompS_reads)}\n"
            )
            mompS2_stat_lines.append(
                f"Per position concistency / inconsistency overview:\n"
            )
            mompS2_stat_lines.append(
                f"""For each position, lists the number of reads where:
            1. Base call matches mompS1 and read is classified as mompS1 (consistent)
            2. Base call matches mompS2 and read is classified as mompS2 (consistent)
            3. Base call does not match the classification of the read (inconsistent)
            4. Base call matches matches mompS2 and read has been identified as a mompS2 border read (consistent)
            5. Base call matches matches mompS1 but read has been identified as a mompS2 border read (inconsistent)
            6. Inconsistency % ( col3 / sum(col1,col2,col3) )
            7. Inconsistency % in border reads ( col5 / sum(col4,col5) )\n
    ---------------------------------------------------------------------------------------\n"""
            )
            for pos, d in position_consistency_dict.items():
                if (
                    min(d["mompS1, consistent"], d["mompS2, consistent"])
                    + d["mompS, inconsistent"]
                ) > 0:
                    inconsistency_percent = round(
                        d["mompS, inconsistent"]
                        / (
                            min(d["mompS1, consistent"], d["mompS2, consistent"])
                            + d["mompS, inconsistent"]
                        )
                        * 100,
                        1,
                    )
                else:
                    inconsistency_percent = None
                if d["border, consistent"] + d["border, inconsistent"] > 0:
                    inconsistency_border_percent = round(
                        d["border, inconsistent"]
                        / (d["border, consistent"] + d["border, inconsistent"])
                        * 100,
                        1,
                    )
                else:
                    inconsistency_border_percent = None
                mompS2_stat_lines.append(
                    str(
                        f"{pos}: {'	'.join([str(x) for x in d.values()])}	{inconsistency_percent}	{inconsistency_border_percent}\n"
                    )
                )
                if d["mompS1, consistent"] < 10 or d["mompS2, consistent"] < 10:
                    low_confidence_warnings.append(
                        f"Low coverage at position {pos}. (mompS1: {d['mompS1, consistent']}, mompS2: {d['mompS2, consistent']})\n"
                    )
                if inconsistency_percent is not None and inconsistency_percent > 5:
                    low_confidence_warnings.append(
                        f"High number of inconsistent base calls at position {pos} ({round(inconsistency_percent,1)}%)\n"
                    )

            filtered_pos_read_call_dict = deepcopy(pos_read_call_dict)
            filtered_read_pos_call_dict = deepcopy(read_pos_call_dict.copy())
            filtered_mompS2_consensus_dict = deepcopy(mompS2_consensus_dict.copy())
            mompS2_readIDs = mompS2_readIDs - inconsistent_mompS_reads
            border_readIDs = border_readIDs - inconsistent_mompS_reads

            # remove reads from mompS-like genes from dictionaries
            if len(positions_to_ignore) > 0:
                for pos in positions_to_ignore:
                    filtered_pos_read_call_dict.pop(pos)
                    filtered_mompS2_consensus_dict.pop(pos)
                for read in read_pos_call_dict:
                    for pos in read_pos_call_dict[read]:
                        if pos in positions_to_ignore:
                            filtered_read_pos_call_dict.pop(read, None)
            logger.info(
                f"{len(positions_to_ignore)} heterozygous positions identified as pseudo-variation from mompS-like genes"
            )
            logger.info(
                f"{len(border_readIDs)} border reads and {len(mompS2_readIDs)} mompS2 reads remaining after filtering mompS-like reads"
            )
            mompS2_stat_lines = mompS2_stat_lines + low_confidence_warnings
            low_confidence_warnings = []
            (
                positions_to_ignore,
                consistent_mompS1_reads,
                consistent_mompS2_reads,
                inconsistent_mompS_reads,
                position_consistency_dict,
            ) = mompS2_mapping_stats(
                filtered_read_pos_call_dict,
                filtered_mompS2_consensus_dict,
                mompS2_readIDs,
                border_readIDs,
            )

            mompS2_stat_lines.append(
                f"---------------------------------------------------------------------------------------\n"
            )
            mompS2_stat_lines.append(f"Iteration 2\n")
            mompS2_stat_lines.append(
                f"{len(border_readIDs)} border reads and {len(mompS2_readIDs)} mompS2 reads remaining after filtering mompS-like reads\n"
            )
            mompS2_stat_lines.append(
                f"Extracting mompS gene from variable positions after removing heterozygozity caused by mompS-like genes\n"
            )
            mompS2_stat_lines.append(
                f"Number of reads consistent with mompS1 on all positions (more than one position): {len(consistent_mompS1_reads)}\n"
            )
            mompS2_stat_lines.append(
                f"Number of reads consistent with mompS2 on all positions (more than one position): {len(consistent_mompS2_reads)}\n"
            )
            mompS2_stat_lines.append(
                f"Inconsistent reads: {len(inconsistent_mompS_reads)}\n"
            )
            for pos, d in position_consistency_dict.items():
                if (
                    min(d["mompS1, consistent"], d["mompS2, consistent"])
                    + d["mompS, inconsistent"]
                ) > 0:
                    inconsistency_percent = round(
                        d["mompS, inconsistent"]
                        / (
                            min(d["mompS1, consistent"], d["mompS2, consistent"])
                            + d["mompS, inconsistent"]
                        )
                        * 100,
                        1,
                    )
                else:
                    inconsistency_percent = None

                if d["border, consistent"] + d["border, inconsistent"] > 0:
                    inconsistency_border_percent = round(
                        d["border, inconsistent"]
                        / (d["border, consistent"] + d["border, inconsistent"])
                        * 100,
                        1,
                    )
                else:
                    inconsistency_border_percent = None
                mompS2_stat_lines.append(
                    f"{pos}: {'	'.join([str(x) for x in d.values()])}	{inconsistency_percent}	{inconsistency_border_percent}\n"
                )
                if d["mompS1, consistent"] < 10 or d["mompS2, consistent"] < 10:
                    low_confidence_warnings.append(
                        f"Low coverage at position {pos}. (mompS1: {d['mompS1, consistent']}, mompS2: {d['mompS2, consistent']})\n"
                    )
            if len(low_confidence_warnings) == 0:
                is_mompS_confident = True
            else:
                is_mompS_confident = False
            mompS2_stat_lines = mompS2_stat_lines + low_confidence_warnings
            # Print fasta file with identified mompS2 allel
            mompS2_allele_seq = print_mompS2_fasta(
                seq_list=seq_list,
                mompS2_consensus_dict=mompS2_consensus_dict,
                output_file=mompS2_fasta_out,
            )
            logger.info(f"mompS2 allel printed to {mompS2_fasta_out}")
        else:
            print("Homozygous mompS2 allele identified")
            is_mompS_confident = True
            # If the two mompS copies are homozygous in allele defining region, simply print the sequence to a fasta file
            logger.info(f"mompS gene copies are homozygous in allele defining region")
            mompS2_allele_seq = "".join(seq_list)[366:718]
            mompS2_seq = DnaSeq(name="mompS2", sequence=mompS2_allele_seq)
            mompS2_seq.print_fasta(mompS2_fasta_out)
            logger.info(f"mompS allel written to {mompS2_fasta_out}")

        # Blast mompS2 consensus fasta against sbt schema references to find best match
        cmd = f"blastn -query {mompS2_fasta_out} -subject {mompS2_allele_fasta} -out {mompS2_allele_blast_out} -outfmt '6 qseqid sseqid pident length slen qstart qend sstart send sseq evalue bitscore' -qcov_hsp_perc 90 -perc_identity 90"
        logger.info(
            f"Blasting mompS2 sequence in {mompS2_fasta_out} against known mompS2 alleles in {mompS2_allele_fasta}"
        )
        stdout, stderr = execute_cmd_and_log(
            cmd=cmd, logger=logger, log_stdout=False, log_stderr=False
        )

        # Parse blast output for best match
        (
            mompS2_allele_number,
            is_mompS_exact_match,
            match_pident,
            match_plen,
            match_seq,
        ) = parse_mompS2_allele_blast(blast_output_file=mompS2_allele_blast_out)
        if is_mompS_exact_match:
            logger.info(f"Exact match found for mompS2 allel: {mompS2_allele_number}")
        else:
            logger.info(
                f"No exact match found for mompS2 allel. Closest allele in database is {mompS2_allele_number} with {match_pident}% identity and {match_plen}% length coverage"
            )
        mompS2_base_stats = [
            f"Best matching allele: {mompS2_allele_number}\n",
            f"Exact match to known allele: {is_mompS_exact_match}\n",
            f"High confidence mompS call: {is_mompS_confident}\n",
            f"Homozygous mompS copies: {not mompS_heterozygous}\n",
            f"Percent identity: {match_pident}\n",
            f"Percent query length: {match_plen}\n",
            f"mompS2 sequence identified in isolate: {mompS2_allele_seq}\n",
            f"Sequence of best matching mompS allele: {match_seq}\n",
        ]
        mompS2_stat_lines = mompS2_base_stats + mompS2_stat_lines

        o = open(mompS2_stats_file, "w")
        o.write("".join(mompS2_stat_lines))
        o.close()

        SBT_allele_dict = {}
        SBT_allele_print_list = []
        logger.info(f"Best allele hits for SBT genes")
        logger.info(f"Gene\tAllele\tPercent_identity\tPercent_length")
        all_alleles_exact = True
        for gene in gene_names:
            if gene == "mompS":
                SBT_allele_dict["mompS"] = mompS2_allele_number
                logger.info(f"mompS	{mompS2_allele_number}	{match_pident}	{match_plen}")
                if is_mompS_exact_match:
                    SBT_allele_print_list.append(mompS2_allele_number)
                elif match_pident > 95 and match_plen > 95:
                    SBT_allele_print_list.append(f"{mompS2_allele_number}*")
                    all_alleles_exact = False
                else:
                    SBT_allele_print_list.append("Novel")
                    all_alleles_exact = False
            else:

                SBT_allele_dict[gene] = allele_dict[gene]
                logger.info(f"{gene}	{'	'.join(detailed_dict[gene])}")
                if (
                    float(detailed_dict[gene][1]) > 99.9
                    and float(detailed_dict[gene][2]) > 99.9
                ):
                    SBT_allele_print_list.append(detailed_dict[gene][0])
                elif match_pident > 95 and match_plen > 95:
                    SBT_allele_print_list.append(f"{detailed_dict[gene][0]}*")
                    all_alleles_exact = False
                else:
                    SBT_allele_print_list.append("Novel")
                    all_alleles_exact = False

        # Load alleles
        best_ST_match, allele_distance = find_best_ST_match_to_alleles(
            isolate_alleles=SBT_allele_dict,
            allele_comp_to_ST=allele_comp_to_ST,
            gene_names=gene_names,
        )
        print_ST = best_ST_match
        notes = []
        if not all_alleles_exact:
            print_ST = f"{best_ST_match}*"

        if allele_distance == 0:
            logging.info(f"Exact match found to allele composition")
            logging.info(f"Sequence type: {best_ST_match}")
            notes.append("Exact ST match")
        else:
            logging.info(f"No exact ST match found to allele composition")
            logging.info(
                f"Nearest match is sequence type {best_ST_match}, with exact allele matches for {7-allele_distance} out of 7 genes"
            )
        if allele_distance > 1:
            print_ST = "NT"
            notes.append(
                f"No ST with <2 allele differences. Closest match is {best_ST_match} with {7-allele_distance} shared alleles"
            )
        if mompS_heterozygous:
            notes.append(f"Heterozygous mompS alleles")
            if is_mompS_confident:
                notes.append(f"High confidence mompS allele call")
            else:
                notes.append(f"Low confidence mompS allele call")
        else:
            notes.append("Homozygous mompS alleles")

        sbt_results_out = open(sbt_results_file, "w")
        sbt_results_out.write(f"sample	ST	{'	'.join(gene_names)}	notes\n")
        if nopath:
            sample_name = Path(assembly_file).name
        else:
            sample_name = assembly_file
        sbt_results_out.write(
            f"{sample_name}	{print_ST}	{'	'.join(SBT_allele_print_list)}	{', '.join(notes)}\n"
        )
        sbt_results_out.close()
        print(f"All results printed to {output_dir}")
        print(f"SBT profile printed to {sbt_results_file}")

    ### Clean up files
    if clean_files:
        Path(sbt_blast_output).unlink(missing_ok=True)
        Path(sam).unlink(missing_ok=True)
        Path(bam).unlink(missing_ok=True)
        Path(sorted_bam).unlink(missing_ok=True)
        Path(f"{sorted_bam}.bai").unlink(missing_ok=True)
        Path(sorted_sam).unlink(missing_ok=True)
        Path(vcf).unlink(missing_ok=True)
        Path(mompS2_allele_blast_out).unlink(missing_ok=True)
        logger.info(f"Cleaned temporary files from {output_dir}")
    logging.shutdown()
    return


def parse_vcf(reference_fasta: Path, vcf_file: Path) -> tuple[list, list, bool]:
    """
    Parses vcf file
    Returns:
    seq_list: A list of base calls on each position (reference sequence, replace with ALT on all homozygous positions, heterozygous positions are untouched)
    ambiguous_positions: A list of all positions with heterozygozity
    mompS_heterozygous: a boolean indicating whether there is heterozigozity in the allele-defining region of mompS

    """
    ambiguous_positions = []
    amb_threshold_percent = 10  # Percent of reads that must belong to minor variant for the position to be considered heterozygous
    QUAL_threshold = (
        100  # QUAL score needed to consider position a true ALT or heterozygous call
    )
    ST_start_pos = 367
    ST_end_pos = 718
    ref_seq = DnaSeq.from_fasta(reference_fasta)
    seq_list = list(ref_seq.sequence)
    mompS_heterozygous = False
    with open(vcf_file) as f:
        for line in f:
            if not line[0] == "#":
                CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, VALUES = (
                    line.rstrip("\n").split("\t")
                )
                POS = int(POS)
                QUAL = float(QUAL)
                AD_index = FORMAT.split(":").index("AD")
                AD_values = VALUES.split(":")[AD_index].split(",")
                AD_values = [int(item) for item in AD_values]
                AD_alt_percent = AD_values[1] / sum(AD_values) * 100
                if AD_alt_percent > amb_threshold_percent and QUAL > QUAL_threshold:
                    if AD_alt_percent < (100 - amb_threshold_percent):
                        ambiguous_positions.append(POS)
                        if POS >= ST_start_pos and POS <= ST_end_pos:
                            mompS_heterozygous = True
                    else:
                        seq_list[POS - 1] = ALT
    return seq_list, ambiguous_positions, mompS_heterozygous


def get_border_reads(sam_file: Path) -> tuple[str, str, list, list]:
    """
    Parse sam file and print all reads covering the border of the reference
    input_file: *.sam
    """
    border_start = 15
    border_end = 972

    border_readIDs = set()
    all_reads = set()

    with open(sam_file) as f:
        for line in f:
            if not line.startswith("@"):
                all_reads.add(line)
                spline = line.rstrip("\n").split("\t")
                readID, FLAG, RNAME, POS, MAPQ, CIGAR, RNEXT, PNEXT, TLEN, SEQ, QUAL = (
                    line.rstrip("\n").split("\t")[:11]
                )
                readID = spline[0]
                read_start = int(POS)
                read_length = sum(
                    map(int, re.findall("(\d+)M", CIGAR))
                )  # Sum to total number of matches in CIGAR string
                read_end = read_start + read_length
                # end_border = border_end-read_length
                if read_start < border_start or read_end > border_end:
                    border_readIDs.add(readID)
    stdout = f"{len(border_readIDs)} mompS2 border reads out of {len(all_reads)} total reads extracted from {sam_file}"
    if len(all_reads) == 0:
        stderr = f"{sam_file} contained no reads mapped to mompS2"
    elif len(border_readIDs) == 0:
        stderr = f"No reads in {sam_file} were mapped to mompS2 border regions. Returning empty list"
    else:
        stderr = None
    return stdout, stderr, border_readIDs, all_reads


def get_positions_from_sam(
    sam_file: Path, ambiguous_positions: list
) -> tuple[dict, dict]:
    """ "
    Loads a sam file and parses for base calls at provided positions in reference.

    INPUTS
    input_file: *.sorted.sam
    ambiguous_positions: [69,74,319,734] (as returned by parse_vcf() function)

    OUTPUT: dictionary of base calls from each read and provided positions in the format:
    {POSITION_1: READ_1: "A",
    POSITION_2: READ_1: "T",
    }
    """
    pos_read_call_dict = {}
    read_pos_call_dict = {}
    for pos in ambiguous_positions:
        pos_read_call_dict[pos] = {}
    with open(sam_file) as f:
        for line in f:
            if not line.startswith("@"):
                QNAME, FLAG, RNAME, POS, MAPQ, CIGAR, RNEXT, PNEXT, TLEN, SEQ, QUAL = (
                    line.rstrip("\n").split("\t")[:11]
                )
                POS = int(POS)
                if (
                    re.search("(\d+)D", CIGAR) is None
                    and re.search("(\d+)D", CIGAR) is None
                ):  # Ignore reads with insertions or deletions
                    match_count = sum(
                        map(int, re.findall("(\d+)M", CIGAR))
                    )  # Sum to total number of matches in CIGAR string
                    if match_count > 30:
                        if QNAME not in read_pos_call_dict:
                            read_pos_call_dict[QNAME] = {}
                        try:
                            start_clip_count = int(re.search("^(\d+)S", CIGAR).group(1))
                        except:
                            start_clip_count = 0
                        for check_pos in ambiguous_positions:
                            in_read_position = check_pos + start_clip_count - POS
                            if check_pos > (POS + start_clip_count) and check_pos < (
                                POS + match_count
                            ):
                                base_call = SEQ[in_read_position : in_read_position + 1]
                                pos_read_call_dict[check_pos][QNAME] = base_call
                                read_pos_call_dict[QNAME][check_pos] = base_call
    return pos_read_call_dict, read_pos_call_dict


def match_positions_to_border_reads(
    pos_read_call_dict: dict, read_pos_call_dict: dict, border_readIDs: list
) -> tuple[dict, dict, dict, list]:
    """
    Iterate over heterozygous positions. Identify base call in mompS2-specific border reads, and assume all reads with same call on that position are mompS2-specific
    """
    all_calls = {}
    mompS2_calls = {}
    mompS2_border_calls = {}
    mompS2_readIDs = set(border_readIDs)
    mompS1_readIDs = set()
    mompS2_consensus_dict = {}
    rev_pos_read_call_dict = dict(reversed(list(pos_read_call_dict.items())))

    for pos, read_base_dict in pos_read_call_dict.items():
        all_calls[pos] = {"A": 0, "C": 0, "G": 0, "T": 0}
        mompS2_calls[pos] = {"A": 0, "C": 0, "G": 0, "T": 0}
        mompS2_border_calls[pos] = {"A": 0, "C": 0, "G": 0, "T": 0}
        for readID, base_call in read_base_dict.items():
            all_calls[pos][base_call] += 1
            if readID in mompS2_readIDs:
                mompS2_calls[pos][base_call] += 1

        ## after looping through all reads, determine mompS2 call for the position
        sorted_mompS2_calls = sorted(
            mompS2_calls[pos], key=mompS2_calls[pos].get, reverse=True
        )
        best_match = sorted_mompS2_calls[0]
        best_match_count = mompS2_calls[pos][best_match]
        best_match_percent = best_match_count / sum(mompS2_calls[pos].values()) * 100
        mompS2_consensus_dict[pos] = [best_match, best_match_count, best_match_percent]
        for readID, base_call in read_base_dict.items():
            if base_call == best_match:
                mompS2_readIDs.add(readID)
            else:
                mompS1_readIDs.add(readID)
        # mompS2_readIDs = mompS2_readIDs-mompS1_readIDs

    for pos, read_base_dict in rev_pos_read_call_dict.items():
        all_calls[pos] = {"A": 0, "C": 0, "G": 0, "T": 0}
        mompS2_calls[pos] = {"A": 0, "C": 0, "G": 0, "T": 0}
        for readID, base_call in read_base_dict.items():
            all_calls[pos][base_call] += 1
            if readID in mompS2_readIDs:
                mompS2_calls[pos][base_call] += 1
                if readID in border_readIDs:
                    mompS2_border_calls[pos][base_call] += 1

        ## after looping through all reads, determine mompS2 call for the position
        sorted_mompS2_calls = sorted(
            mompS2_calls[pos], key=mompS2_calls[pos].get, reverse=True
        )
        best_match = sorted_mompS2_calls[0]
        best_match_count = mompS2_calls[pos][best_match]
        best_match_percent = best_match_count / sum(mompS2_calls[pos].values()) * 100
        mompS2_consensus_dict[pos] = [best_match, best_match_count, best_match_percent]
        for readID, base_call in read_base_dict.items():
            if base_call == best_match:
                mompS2_readIDs.add(readID)
            else:
                mompS1_readIDs.add(readID)

    return (
        mompS2_consensus_dict,
        all_calls,
        mompS2_calls,
        mompS2_border_calls,
        mompS2_readIDs,
    )


def mompS2_mapping_stats(
    read_pos_call_dict: dict[dict],
    mompS2_consensus_dict: dict[list],
    mompS2_readIDs: list,
    border_readIDs: list,
):
    percent_inconsistency_threshold = 5
    positions_to_ignore = set()
    consistent_mompS1_reads = set()
    consistent_mompS2_reads = set()
    inconsistent_mompS_reads = set()
    position_consistency_dict = {}

    for pos in mompS2_consensus_dict:
        position_consistency_dict[pos] = {
            "mompS1, consistent": 0,
            "mompS2, consistent": 0,
            "mompS, inconsistent": 0,
            "border, consistent": 0,
            "border, inconsistent": 0,
        }

    n = 0
    for readID, pos_base_dict in read_pos_call_dict.items():
        is_border_read = readID in border_readIDs
        if len(pos_base_dict) > 1 or is_border_read:
            is_mompS2 = True
            is_consistent = True
            for pos, base in pos_base_dict.items():
                if base == mompS2_consensus_dict[pos][0]:
                    if readID in mompS2_readIDs:
                        position_consistency_dict[pos]["mompS2, consistent"] += 1
                        if is_border_read:
                            position_consistency_dict[pos]["border, consistent"] += 1
                    else:
                        is_consistent = False
                        is_mompS2 = False
                        position_consistency_dict[pos]["mompS, inconsistent"] += 1
                else:
                    if readID in mompS2_readIDs:
                        is_consistent = False
                        position_consistency_dict[pos]["mompS, inconsistent"] += 1
                        if is_border_read:
                            position_consistency_dict[pos]["border, inconsistent"] += 1
                    else:
                        is_mompS2 = False
                        position_consistency_dict[pos]["mompS1, consistent"] += 1
            if is_mompS2:
                if is_consistent:
                    consistent_mompS2_reads.add(readID)
                else:
                    inconsistent_mompS_reads.add(readID)
            else:
                if is_consistent:
                    consistent_mompS1_reads.add(readID)
                else:
                    inconsistent_mompS_reads.add(readID)
    for pos, d in position_consistency_dict.items():
        if (
            min(d["mompS1, consistent"], d["mompS2, consistent"])
            + d["mompS, inconsistent"]
        ) > 0:
            inconsistency_percent = (
                d["mompS, inconsistent"]
                / (
                    min(d["mompS1, consistent"], d["mompS2, consistent"])
                    + d["mompS, inconsistent"]
                )
                * 100
            )
        else:
            inconsistency_percent = None
            positions_to_ignore.add(pos)

        if d["border, consistent"] + d["border, inconsistent"] > 0:
            inconsistency_border_percent = (
                d["border, inconsistent"]
                / (d["border, consistent"] + d["border, inconsistent"])
                * 100
            )
        else:
            inconsistency_border_percent = None
            positions_to_ignore.add(pos)

        if inconsistency_percent is None:
            positions_to_ignore.add(pos)
        elif inconsistency_percent > percent_inconsistency_threshold:
            positions_to_ignore.add(pos)
        elif inconsistency_border_percent is None:
            positions_to_ignore.add(pos)
        elif inconsistency_border_percent > percent_inconsistency_threshold:
            positions_to_ignore.add(pos)

    return (
        positions_to_ignore,
        consistent_mompS1_reads,
        consistent_mompS2_reads,
        inconsistent_mompS_reads,
        position_consistency_dict,
    )


def print_mompS2_fasta(
    seq_list: list, mompS2_consensus_dict: dict, output_file: Path
) -> DnaSeq:
    # ref_seq = DnaSeq.from_fasta(mompS2_ref)
    ST_start_pos = 367
    ST_end_pos = 718
    # seq_list = list(ref_seq.sequence)
    for pos, base_counts in mompS2_consensus_dict.items():
        seq_list[pos - 1] = base_counts[0]
    seq = "".join(seq_list[ST_start_pos - 1 : ST_end_pos])
    mompS2_seq = DnaSeq("mompS2", seq)
    mompS2_seq.print_fasta(output_file)
    return seq


def parse_mompS2_allele_blast(blast_output_file) -> tuple[str, bool, float, float]:
    blast_df = pd.read_csv(
        blast_output_file,
        sep="\t",
        header=None,
        names="qseqid sseqid pident length slen qstart qend sstart send sseq evalue bitscore".split(
            " "
        ),
    )
    blast_besthit = (
        blast_df.sort_values(by=["bitscore"], ascending=False).iloc[0].to_dict()
    )
    plen = blast_besthit["length"] / blast_besthit["slen"] * 100

    if plen >= 100 and blast_besthit["pident"] >= 100:
        is_exact_match = True
    else:
        is_exact_match = False
    bestmatch_allele_number = blast_besthit["sseqid"].split("_")[1]
    pident = round(blast_besthit["pident"], 1)
    plen = round(plen, 1)
    sseq = blast_besthit["sseq"]
    return bestmatch_allele_number, is_exact_match, pident, plen, sseq


def get_alleles_from_blast(sbt_profile_tsv):
    alleles, allele_comp_dict = load_allele_comp_to_ST_dict(sbt_profile_tsv)


def run_sbt_blast(
    assembly_file: Path, sbt_allele_blast_db: Path, blast_out: Path
) -> None:
    cmd = f"blastn -query {assembly_file} -sbt {sbt_allele_blast_db} -out {blast_out} -qcov_hsp_perc 80 -perc_identity 95 -num_alignments 10000 -outfmt '6 qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore'"
    os.system(cmd)
    return None


def extract_blast_sbt(blast_out: Path) -> tuple[dict, dict]:
    """
    Parse blast output tsv for best matching alleles


    """
    allele_dict = {}
    detailed_dict = {}
    if os.path.exists(blast_out):
        blast_df = pd.read_csv(blast_out, sep="\t", header=None)
        blast_df.columns = "qseqid sseqid pident length qlen qstart qend sstart send sseq evalue bitscore".split(
            " "
        )
        blast_df["plen"] = blast_df["length"] / blast_df["qlen"] * 100
        blast_df[["gene", "allele"]] = blast_df["qseqid"].str.split("_", expand=True)
        blast_df_unique = (
            blast_df.sort_values(by=["bitscore"], ascending=False)
            .groupby("gene")
            .first()
        )
        for gene, d in blast_df_unique.to_dict(orient="index").items():
            if gene == "neuAh" and not "neuA" in allele_dict:
                allele_dict["neuA"] = d["allele"]
                detailed_dict["neuA"] = (d["allele"], str(d["pident"]), str(d["plen"]))
            else:
                allele_dict[gene] = d["allele"]
                detailed_dict[gene] = (d["allele"], str(d["pident"]), str(d["plen"]))
    else:
        print(f"No blast output found at {blast_out}", file=sys.stderr)
    return allele_dict, detailed_dict


def load_allele_comp_to_ST_dict(sbt_profile_tsv) -> tuple[list, dict]:
    """
    Loads tsv of MLST profiles and returns a list of gene names used in the scheme (header line) and a dictionary matching a string of allele numbers to an ST
    i.e.
    {"1_4_3_1_1_1_1": "1",
     "6_10_10_3_19_4_9": "2",
     "1_4_3_1_14_1_9": "3}
    """

    allele_comp_dict = {}
    with open(sbt_profile_tsv) as f:
        firstline_flag = True
        for line in f:
            line = line.rstrip("\n").split("\t")
            if firstline_flag:
                firstline_flag = False
                gene_names = line[1:]
            else:
                allele_comp_dict["_".join(line[1:])] = line[0]
    return gene_names, allele_comp_dict


def find_best_ST_match_to_alleles(
    isolate_alleles: dict, allele_comp_to_ST: dict, gene_names: list
):
    isolate_alleles_sorted = []
    for gene in gene_names:
        if gene in isolate_alleles:
            isolate_alleles_sorted.append(isolate_alleles[gene])
        else:
            isolate_alleles_sorted.append("-")
    allele_comp_string = "_".join(isolate_alleles_sorted)
    allele_dist_dict = {}
    if allele_comp_string in allele_comp_to_ST:
        sequence_type = allele_comp_to_ST[allele_comp_string]
        allele_distance = 0
    else:
        for allele_comp, ST in allele_comp_to_ST.items():
            allele_dist = 0
            allele_comp_list = allele_comp.split("_")
            for i, allele in enumerate(allele_comp_list):
                if not allele == isolate_alleles_sorted[i]:
                    allele_dist += 1
            allele_dist_dict[ST] = allele_dist
        sorted_dists = {
            k: v for k, v in sorted(allele_dist_dict.items(), key=lambda item: item[1])
        }
        sequence_type, allele_distance = list(sorted_dists.items())[0]
    return sequence_type, allele_distance


def get_depth_from_sam(sam_file: Path, min_depth: int = 10) -> tuple[bool, dict, dict]:
    ST_start_pos = 367
    ST_end_pos = 718
    low_depth_positions_in_ST_region = {}
    low_depth_positions_outside_ST_region = {}
    process = subprocess.Popen(
        f"samtools depth {sam_file}",
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        shell=True,
        encoding="utf-8",
    )
    stdout, stderr = process.communicate()
    error_msg = False
    try:
        for line in stdout.split("\n"):
            line_list = line.split("\t")
            if len(line_list) >= 3:
                pos = int(line_list[1])
                depth = int(line_list[2])
                if depth < min_depth:
                    if pos >= ST_start_pos and pos <= ST_end_pos:
                        low_depth_positions_in_ST_region[pos] = depth
                    else:
                        low_depth_positions_outside_ST_region[pos] = depth
    except Exception as e:
        error_msg = e

    return (
        error_msg,
        low_depth_positions_in_ST_region,
        low_depth_positions_outside_ST_region,
    )

# %% ../nbs/02_leg_sbt.ipynb 12
from fastcore.script import call_parse


@call_parse
def cli(
    assembly_file: Path = None,  # Path to genome assembly
    r1_file: Path = None,  # Path to forward read file
    r2_file: Path = None,  # Path to reverse read file
    output_folder: Path = None,  # Ouput folder
    output_prefix: str = "legionella",  # Output prefix
    config_file: Path = None,  # config file to set env vars from
) -> None:
    """
    Run legionella SBT
    """
    package_dir = core.get_package_dir()
    config = core.get_config(config_file)  # Set env vars and get config variables
    run_sbt_typing(
        assembly_file=assembly_file,
        r1_file=r1_file,
        r2_file=r2_file,
        mompS2_reference_fasta=package_dir.joinpath(
            config["resources"]["mompS2_reference_fasta"]
        ),
        mompS2_allele_fasta=package_dir.joinpath(
            config["resources"]["mompS2_allele_fasta"]
        ),
        sbt_allele_blast_db=package_dir.joinpath(
            config["resources"]["sbt_allele_blast_db"]
        ),
        sbt_profiles_tsv=package_dir.joinpath(config["resources"]["sbt_profiles_tsv"]),
        output_dir=output_folder,
        output_prefix=output_prefix,
    )
