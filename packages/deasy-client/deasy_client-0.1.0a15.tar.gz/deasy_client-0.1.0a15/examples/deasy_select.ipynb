{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deasy-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "from deasy_client import Deasy\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deasy Meta-Filter Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Define tags you want to filter on and instantiate the Deasy Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deasy_api_key = os.environ[\"DEASY_API_KEY\"] # You get in the Deasy Tokens Dashboard\n",
    "username = \"<username>\" # Your Deasy username\n",
    "\n",
    "vdb_profile_name = \"<vdb_profile_name>\" # name of the vdb profile/datasource you want to use\n",
    "\n",
    "client = Deasy(\n",
    "    x_user=username,\n",
    "    x_token=deasy_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Use Deasy SDK to get tag schemas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_schemas = client.tags.list().tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Ask your question. You can define what you expect as responses in the \"columns\" attribute \n",
    "###### id, filename, text, dense->dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I use Deasy's tag schemas and metadata filtering to improve the relevance of my retrieval results?\"\n",
    "\n",
    "\n",
    "response = client.metadata.deasy_select.query(\n",
    "    query=question,\n",
    "    vdb_profile_name=vdb_profile_name,\n",
    "    columns=[\"id\", \"filename\", \"text\", \"dense\"],\n",
    "    tag_schemas=tag_schemas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. (OPTIONAL) In case you want to rerank your results, you must use fastembed with a vector dimension of 384 defined below out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "\n",
    "embedder =TextEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    ")\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    if len(v1.shape) > 1:\n",
    "        v1 = v1.flatten()\n",
    "    if len(v2.shape) > 1:\n",
    "        v2 = v2.flatten()\n",
    "    \n",
    "    dot_product = np.dot(v1, v2)\n",
    "    \n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "def rerank_results(question, results):\n",
    "    question_vector = list(embedder.embed(question))[0] \n",
    "    \n",
    "    doc_vectors = []\n",
    "    for doc in results:\n",
    "        dense_str = doc['dense'].strip('[]').split(',')\n",
    "        dense_array = np.array([float(x) for x in dense_str])\n",
    "        doc_vectors.append(dense_array)\n",
    "    \n",
    "    similarities = [\n",
    "        cosine_similarity(question_vector, doc_array)\n",
    "        for doc_array in doc_vectors\n",
    "    ]\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    top_results = [results[i] for i in top_indices]\n",
    "    return top_results\n",
    "\n",
    "\n",
    "reranked_response = rerank_results(question, response[\"results\"])\n",
    "\n",
    "reranked_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
