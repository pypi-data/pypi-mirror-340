"""
pip install jieba emoji
"""
import re
import emoji
import jieba
from typing import List

# ç¤ºä¾‹åœç”¨è¯åˆ—è¡¨ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
stopwords = {"çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½",
             "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°",
             "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™"}


def preprocess_text(text):
    # è‹±æ–‡æ ‡ç‚¹è½¬ä¸­æ–‡æ ‡ç‚¹
    punctuation_map = {
        ',': 'ï¼Œ',
        '.': 'ã€‚',
        '!': 'ï¼',
        '?': 'ï¼Ÿ',
        ';': 'ï¼›',
        ':': 'ï¼š',
        '"': 'â€œ',
        "'": 'â€˜',
        '(': 'ï¼ˆ',
        ')': 'ï¼‰',
        '[': 'ã€',
        ']': 'ã€‘',
        '{': 'ã€Š',
        '}': 'ã€‹',
    }
    for eng_punct, zh_punct in punctuation_map.items():
        text = text.replace(eng_punct, zh_punct)

    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)

    # å»é™¤URL
    text = re.sub(r'http[s]?://\S+', '', text)

    # å»é™¤è¡¨æƒ…ç¬¦å·
    text = emoji.replace_emoji(text, replace='')

    # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

    # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·
    ...

    # å»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def remove_stopwords(words):
    # å»é™¤åœç”¨è¯
    return [word for word in words if word not in stopwords]


def segment_and_clean(text):
    # åˆ†è¯
    words = jieba.lcut(text)
    # å»é™¤åœç”¨è¯
    clean_words = remove_stopwords(words)
    return clean_words


def recognize_chinese_character(text: str) -> List[str]:
    """
    ä¸­æ–‡å­—ç¬¦å¤§è‡´ä½äº Unicode èŒƒå›´ \u4e00-\u9fffã€‚
    ä¸­æ–‡æ ‡ç‚¹åˆ™åˆ†å¸ƒåœ¨å¤šä¸ªåŒºé—´ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
        â€¢	\u3000-\u303fï¼šä¸­æ–‡æ ‡ç‚¹ç¬¦å·
        â€¢	\uff00-\uffefï¼šå…¨è§’ASCIIã€å…¨è§’æ ‡ç‚¹
    """
    # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…ä¸­æ–‡å­—ç¬¦å’Œä¸­æ–‡æ ‡ç‚¹ç¬¦å·
    pattern = re.compile(r'[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef]+')

    # ä½¿ç”¨findallæ–¹æ³•æ‰¾åˆ°æ‰€æœ‰åŒ¹é…é¡¹
    matches = pattern.findall(text)
    return matches

def recognize_chinese_characters_exclude_punctuation(text: str) -> List[str]:
    """
    è¯†åˆ«ä¸­æ–‡æ–‡å­—ï¼Œä¸åŒ…æ‹¬ä¸­æ–‡æ ‡ç‚¹ã€‚
    ä¸­æ–‡å­—ç¬¦å¤§è‡´ä½äº Unicode èŒƒå›´ \u4e00-\u9fffã€‚
    """
    # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥ä»…åŒ¹é…ä¸­æ–‡å­—ç¬¦
    pattern = re.compile(r'[\u4e00-\u9fff]+')

    # ä½¿ç”¨findallæ–¹æ³•æ‰¾åˆ°æ‰€æœ‰åŒ¹é…é¡¹
    matches = pattern.findall(text)
    return matches



if __name__ == '__main__':
    # ç¤ºä¾‹æ–‡æœ¬
    sample_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ï¼šå…¨è§’ç¬¦å·ï¼‹æ±‰å­—++ã€‚è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼ŒåŒ…å«HTMLæ ‡ç­¾<a href='https://example.com'>é“¾æ¥</a>ã€è¡¨æƒ…ğŸ˜Šå’Œç‰¹æ®Šå­—ç¬¦ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰ã€‚"

    # # é¢„å¤„ç†æ–‡æœ¬
    # cleaned_text = preprocess_text(sample_text)
    # print("é¢„å¤„ç†åçš„æ–‡æœ¬ï¼š", cleaned_text)
    #
    # # åˆ†è¯å¹¶å»é™¤åœç”¨è¯
    # clean_words = segment_and_clean(cleaned_text)
    # print("åˆ†è¯å¹¶å»é™¤åœç”¨è¯åçš„è¯è¯­ï¼š", clean_words)

    # è¯†åˆ«ä¸­æ–‡å­—ç¬¦
    x = recognize_chinese_character(sample_text)
    print(x)
