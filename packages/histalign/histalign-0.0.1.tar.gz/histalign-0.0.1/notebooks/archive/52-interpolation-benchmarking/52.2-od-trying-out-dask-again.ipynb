{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-07T12:33:59.619580Z",
     "start_time": "2024-11-07T12:33:52.222856Z"
    }
   },
   "source": [
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from helper_loader import *\n",
    "from histalign.backend.registration.alignment import (\n",
    "    INTERPOLATED_VOLUMES_CACHE_DIRECTORY,\n",
    "    _module_logger,\n",
    "    build_alignment_volume,\n",
    ")\n",
    "\n",
    "set_log_level(\"DEBUG\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:34:04.543595Z",
     "start_time": "2024-11-07T12:34:02.109986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit=\"2GB\")\n",
    "client"
   ],
   "id": "23c3809efb83d773",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:34:21.361556Z",
     "start_time": "2024-11-07T12:34:20.644586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alignment_path = Path(\n",
    "    \"/home/ediun/git/histalign/projects/project_cortical_depth/93e6cae680\"\n",
    ")\n",
    "\n",
    "array = build_alignment_volume(alignment_path, return_raw_array=True)"
   ],
   "id": "5695103cf272d167",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ediun/git/histalign/projects/project_cortical_depth/93e6cae680'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m alignment_path \u001B[38;5;241m=\u001B[39m Path(\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/ediun/git/histalign/projects/project_cortical_depth/93e6cae680\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m )\n\u001B[0;32m----> 5\u001B[0m array \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_alignment_volume\u001B[49m\u001B[43m(\u001B[49m\u001B[43malignment_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_raw_array\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/git/histalign/src/histalign/backend/registration/alignment.py:50\u001B[0m, in \u001B[0;36mbuild_alignment_volume\u001B[0;34m(alignment_directory, use_cache, return_raw_array)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(alignment_directory, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     48\u001B[0m     alignment_directory \u001B[38;5;241m=\u001B[39m Path(alignment_directory)\n\u001B[0;32m---> 50\u001B[0m targets \u001B[38;5;241m=\u001B[39m \u001B[43mgather_alignment_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43malignment_directory\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m targets_hash \u001B[38;5;241m=\u001B[39m generate_hash_from_targets(targets)\n\u001B[1;32m     53\u001B[0m cache_path \u001B[38;5;241m=\u001B[39m ALIGNMENT_VOLUMES_CACHE_DIRECTORY \u001B[38;5;241m/\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtargets_hash\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npz\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/git/histalign/src/histalign/backend/io/__init__.py:115\u001B[0m, in \u001B[0;36mgather_alignment_paths\u001B[0;34m(alignment_directory)\u001B[0m\n\u001B[1;32m    111\u001B[0m     alignment_directory \u001B[38;5;241m=\u001B[39m Path(alignment_directory)\n\u001B[1;32m    113\u001B[0m paths \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 115\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m alignment_directory\u001B[38;5;241m.\u001B[39miterdir():\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re\u001B[38;5;241m.\u001B[39mfullmatch(ALIGNMENT_FILE_NAME_PATTERN, file\u001B[38;5;241m.\u001B[39mname) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/pathlib.py:1017\u001B[0m, in \u001B[0;36mPath.iterdir\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21miterdir\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;124;03m    result for the special paths '.' and '..'.\u001B[39;00m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1017\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1018\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m}:\n\u001B[1;32m   1019\u001B[0m             \u001B[38;5;66;03m# Yielding a path object for these makes little sense\u001B[39;00m\n\u001B[1;32m   1020\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/ediun/git/histalign/projects/project_cortical_depth/93e6cae680'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:07:56.235921Z",
     "start_time": "2024-11-07T12:07:56.223357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def generate_hash_from_targets(targets: list[Path]) -> str:\n",
    "#     return hashlib.md5(\"\".join(map(str, targets)).encode(\"UTF-8\")).hexdigest()\n",
    "#\n",
    "#\n",
    "# def interpolate_sparse_3d_array(\n",
    "#     array: np.ndarray,\n",
    "#     reference_mask: Optional[np.ndarray] = None,\n",
    "#     pre_masked: bool = False,\n",
    "#     kernel: str = \"multiquadric\",\n",
    "#     neighbours: int = 27,\n",
    "#     epsilon: int = 1,\n",
    "#     degree: Optional[int] = None,\n",
    "#     chunk_size: Optional[int] = 1_000_000,\n",
    "#     recursive: bool = False,\n",
    "#     use_cache: bool = False,\n",
    "#     alignment_directory: str | Path = \"\",\n",
    "#     mask_name: str = \"\",\n",
    "# ) -> np.ndarray:\n",
    "#     start_time = time.perf_counter()\n",
    "#\n",
    "#     if use_cache and not alignment_directory:\n",
    "#         raise ValueError(\n",
    "#             \"Cannot use cache without 'alignment_directory' identifying information.\"\n",
    "#         )\n",
    "#     if use_cache and reference_mask is not None and not mask_name:\n",
    "#         raise ValueError(\n",
    "#             \"Cannot use cache with reference mask but no 'mask_name' \"\n",
    "#             \"identifying information.\"\n",
    "#         )\n",
    "#     if isinstance(alignment_directory, str):\n",
    "#         alignment_directory = Path(alignment_directory)\n",
    "#\n",
    "#     if reference_mask is not None and (array_shape := array.shape) != (\n",
    "#         reference_shape := reference_mask.shape\n",
    "#     ):\n",
    "#         raise ValueError(\n",
    "#             f\"Array and reference mask have different shapes \"\n",
    "#             f\"({array_shape} vs {reference_shape}).\"\n",
    "#         )\n",
    "#\n",
    "#     # Mask the array if necessary\n",
    "#     if reference_mask is not None and not pre_masked:\n",
    "#         array = np.where(reference_mask, array, 0)\n",
    "#\n",
    "#     cache_hash = generate_hash_from_targets(gather_alignment_paths(alignment_directory))\n",
    "#     mask_name = \"-\".join(mask_name.split(\" \")).lower()\n",
    "#     cache_path = (\n",
    "#         INTERPOLATED_VOLUMES_CACHE_DIRECTORY\n",
    "#         / f\"{cache_hash}{f'_{mask_name}' if reference_mask is not None else ''}.npz\"\n",
    "#     )\n",
    "#     if cache_path.exists() and use_cache:\n",
    "#         _module_logger.debug(\"Found cached array. Loading from file.\")\n",
    "#\n",
    "#         return np.load(cache_path)[\"array\"]\n",
    "#\n",
    "#     interpolated_array = array.copy()\n",
    "#     interpolated_array = interpolated_array.astype(np.float64)\n",
    "#\n",
    "#     if reference_mask is None:\n",
    "#         # Interpolate the whole grid\n",
    "#         target_coordinates = tuple(\n",
    "#             array.flatten().astype(int)\n",
    "#             for array in np.meshgrid(\n",
    "#                 np.linspace(\n",
    "#                     0, interpolated_array.shape[0] - 1, interpolated_array.shape[0]\n",
    "#                 ),\n",
    "#                 np.linspace(\n",
    "#                     0, interpolated_array.shape[1] - 1, interpolated_array.shape[1]\n",
    "#                 ),\n",
    "#                 np.linspace(\n",
    "#                     0, interpolated_array.shape[2] - 1, interpolated_array.shape[2]\n",
    "#                 ),\n",
    "#                 indexing=\"ij\",\n",
    "#             )\n",
    "#         )\n",
    "#     else:\n",
    "#         # Interpolate only non-zero coordinates of mask\n",
    "#         target_coordinates = np.nonzero(reference_mask)\n",
    "#     target_points = np.array(target_coordinates).T\n",
    "#\n",
    "#     if chunk_size is None:\n",
    "#         chunk_size = target_points.shape[0]\n",
    "#\n",
    "#     _module_logger.info(\n",
    "#         f\"Starting interpolation with parameters \"\n",
    "#         f\"{{\"\n",
    "#         f\"kernel: {kernel}, \"\n",
    "#         f\"neighbours: {neighbours}, \"\n",
    "#         f\"epsilon: {epsilon}, \"\n",
    "#         f\"degree: {degree}, \"\n",
    "#         f\"chunk size: {chunk_size:,}, \"\n",
    "#         f\"recursive: {recursive}\"\n",
    "#         f\"}}.\"\n",
    "#     )\n",
    "#\n",
    "#     failed_chunks = []\n",
    "#     previous_target_size = target_points.shape[0]\n",
    "#     while True:\n",
    "#         known_coordinates = np.nonzero(interpolated_array)\n",
    "#         known_points = np.array(known_coordinates).T\n",
    "#\n",
    "#         known_values = array[known_coordinates]\n",
    "#\n",
    "#         interpolator = RBFInterpolator(\n",
    "#             known_points,\n",
    "#             known_values,\n",
    "#             kernel=kernel,\n",
    "#             neighbors=neighbours,\n",
    "#             epsilon=epsilon,\n",
    "#             degree=degree,\n",
    "#         )\n",
    "#\n",
    "#         def interpolation_function(chunk: np.ndarray) -> np.ndarray:\n",
    "#             try:\n",
    "#                 interpolated_data = interpolator(chunk)\n",
    "#             except np.linalg.LinAlgError:\n",
    "#                 interpolated_data = np.zeros(shape=(chunk.shape[0],), dtype=np.float64)\n",
    "#\n",
    "#             return interpolated_data\n",
    "#\n",
    "#         chunk_start = 0\n",
    "#         chunk_end = chunk_size\n",
    "#         chunk_index = 1\n",
    "#         chunk_count = math.ceil(target_points.shape[0] / chunk_size)\n",
    "#         while chunk_start < target_points.shape[0]:\n",
    "#             _module_logger.info(\n",
    "#                 f\"Interpolating chunk {chunk_index}/{chunk_count} \"\n",
    "#                 f\"({chunk_index / chunk_count:.0%}).\"\n",
    "#             )\n",
    "#\n",
    "#             chunk_coordinates = tuple(\n",
    "#                 coordinate[chunk_start:chunk_end] for coordinate in target_coordinates\n",
    "#             )\n",
    "#             chunk_points = target_points[chunk_start:chunk_end]\n",
    "#             chunk_points = da.from_array(chunk_points)\n",
    "#\n",
    "#             try:\n",
    "#                 interpolated_array[chunk_coordinates] = interpolator(chunk_points)\n",
    "#             except np.linalg.LinAlgError:\n",
    "#                 failed_chunks.append([chunk_start, chunk_end])\n",
    "#                 _module_logger.info(f\"Failed to interpolate chunk {chunk_index}.\")\n",
    "#\n",
    "#             chunk_start += chunk_size\n",
    "#             chunk_end += chunk_size\n",
    "#             chunk_index += 1\n",
    "#\n",
    "#         if not recursive or len(failed_chunks) == 0:\n",
    "#             break\n",
    "#\n",
    "#         # Prepare the next loop\n",
    "#         target_coordinates = tuple(\n",
    "#             np.concatenate(\n",
    "#                 [target_coordinate[start:end] for start, end in failed_chunks]\n",
    "#             )\n",
    "#             for target_coordinate in target_coordinates\n",
    "#         )\n",
    "#         target_points = np.array(target_coordinates).T\n",
    "#         failed_chunks = []\n",
    "#\n",
    "#         # Avoid infinitely looping\n",
    "#         if previous_target_size == target_points.shape[0]:\n",
    "#             _module_logger.error(\n",
    "#                 f\"Interpolation is not fully solvable with current combination of \"\n",
    "#                 f\"kernel, neighbours parameter and chunk size. \"\n",
    "#                 f\"Returning current result.\"\n",
    "#             )\n",
    "#             break\n",
    "#         previous_target_size = target_points.shape[0]\n",
    "#\n",
    "#         _module_logger.info(\n",
    "#             f\"There were {len(failed_chunks)} failed chunks of size {chunk_size}. \"\n",
    "#             f\"Recursing with newly interpolated data.\"\n",
    "#         )\n",
    "#\n",
    "#     total_time = time.perf_counter() - start_time\n",
    "#     total_hours, remaining_time = divmod(total_time, 3600)\n",
    "#     total_minutes, total_seconds = divmod(remaining_time, 60)\n",
    "#     time_string = (\n",
    "#         f\"{f'{total_hours:.0f}h' if total_hours else ''}\"\n",
    "#         f\"{f'{total_minutes:>2.0f}m' if total_minutes else ''}\"\n",
    "#         f\"{total_seconds:>2.0f}s\"\n",
    "#     )\n",
    "#     _module_logger.info(f\"Finished interpolation in {time_string}.\")\n",
    "#\n",
    "#     if use_cache:\n",
    "#         _module_logger.debug(\"Caching interpolated array to file.\")\n",
    "#         os.makedirs(INTERPOLATED_VOLUMES_CACHE_DIRECTORY, exist_ok=True)\n",
    "#         np.savez_compressed(cache_path, array=interpolated_array)\n",
    "#\n",
    "#     return interpolated_array"
   ],
   "id": "2b564b2a0d2a2d67",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:07:56.294988Z",
     "start_time": "2024-11-07T12:07:56.282922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tuple_all_coordinates(array_shape: tuple[int, ...]) -> tuple[np.ndarray, ...]:\n",
    "    assert max(array_shape) < 2**16 - 1, \"Supported coordinate dtype is np.uint16\"\n",
    "\n",
    "    return tuple(\n",
    "        array.flatten().astype(int)\n",
    "        for array in np.meshgrid(\n",
    "            *[\n",
    "                np.linspace(0, array_shape[i] - 1, array_shape[i], dtype=np.uint16)\n",
    "                for i in range(len(array_shape))\n",
    "            ],\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "    )"
   ],
   "id": "f4689b9ee26aeb46",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:07:56.366498Z",
     "start_time": "2024-11-07T12:07:56.343124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def interpolate_dask(array: np.ndarray) -> np.ndarray:\n",
    "    interpolated_array = array.copy().astype(np.float64)\n",
    "\n",
    "    known_coordinates = np.nonzero(interpolated_array)\n",
    "    known_points = np.array(known_coordinates).T\n",
    "    known_points = da.from_array(known_points)\n",
    "\n",
    "    known_values = array[known_coordinates]\n",
    "    known_values = da.from_array(known_values)\n",
    "\n",
    "    interpolator = RBFInterpolator(\n",
    "        known_points,\n",
    "        known_values,\n",
    "        kernel=\"multiquadric\",\n",
    "        neighbors=16,\n",
    "        epsilon=1,\n",
    "        degree=None,\n",
    "    )\n",
    "\n",
    "    def interpolation_function(chunk: np.ndarray) -> np.ndarray:\n",
    "        try:\n",
    "            interpolated_data = interpolator(chunk)\n",
    "        except np.linalg.LinAlgError:\n",
    "            interpolated_data = np.zeros(shape=(chunk.shape[0],), dtype=np.float64)\n",
    "\n",
    "        return interpolated_data\n",
    "\n",
    "    target_coordinates = get_tuple_all_coordinates(interpolated_array.shape)\n",
    "    target_points = np.array(target_coordinates, np.uint16).T\n",
    "    target_points = da.from_array(target_points)\n",
    "\n",
    "    return da.map_blocks(\n",
    "        interpolation_function, target_points, drop_axis=1, dtype=np.float64\n",
    "    )"
   ],
   "id": "27babddce019c309",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T12:13:37.436541Z",
     "start_time": "2024-11-07T12:07:56.404098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "interpolated_data = interpolate_dask(array)\n",
    "interpolated_data.compute()"
   ],
   "id": "5784751204bcec62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediun/.local/share/hatch/env/virtual/histalign/oFfvMJyE/histalign/lib/python3.10/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 461.07 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "2024-11-07 12:08:42,437 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.32 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:08:58,885 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 1.49 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:10:06,147 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.31 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:10:22,694 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 1.49 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:11:28,958 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.32 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:11:45,843 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 1.49 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:12:51,759 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.31 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:13:04,678 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.30 GiB -- Worker memory limit: 1.86 GiB\n",
      "2024-11-07 12:13:08,810 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 1.49 GiB -- Worker memory limit: 1.86 GiB\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task ('interpolation_function-624c9e9c5b974a47ea5cd0aa38afe857', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:39071. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKilledWorker\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n",
      "File \u001B[0;32m~/.local/share/hatch/env/virtual/histalign/oFfvMJyE/histalign/lib/python3.10/site-packages/dask/base.py:372\u001B[0m, in \u001B[0;36mDaskMethodsMixin.compute\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    349\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute this dask collection\u001B[39;00m\n\u001B[1;32m    350\u001B[0m \n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;124;03m    dask.compute\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 372\u001B[0m     (result,) \u001B[38;5;241m=\u001B[39m \u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraverse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/.local/share/hatch/env/virtual/histalign/oFfvMJyE/histalign/lib/python3.10/site-packages/dask/base.py:660\u001B[0m, in \u001B[0;36mcompute\u001B[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001B[0m\n\u001B[1;32m    657\u001B[0m     postcomputes\u001B[38;5;241m.\u001B[39mappend(x\u001B[38;5;241m.\u001B[39m__dask_postcompute__())\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m shorten_traceback():\n\u001B[0;32m--> 660\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mschedule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdsk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    662\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m repack([f(r, \u001B[38;5;241m*\u001B[39ma) \u001B[38;5;28;01mfor\u001B[39;00m r, (f, a) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(results, postcomputes)])\n",
      "File \u001B[0;32m~/.local/share/hatch/env/virtual/histalign/oFfvMJyE/histalign/lib/python3.10/site-packages/distributed/client.py:2417\u001B[0m, in \u001B[0;36mClient._gather\u001B[0;34m(self, futures, errors, direct, local_worker)\u001B[0m\n\u001B[1;32m   2415\u001B[0m     exception \u001B[38;5;241m=\u001B[39m st\u001B[38;5;241m.\u001B[39mexception\n\u001B[1;32m   2416\u001B[0m     traceback \u001B[38;5;241m=\u001B[39m st\u001B[38;5;241m.\u001B[39mtraceback\n\u001B[0;32m-> 2417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exception\u001B[38;5;241m.\u001B[39mwith_traceback(traceback)\n\u001B[1;32m   2418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2419\u001B[0m     bad_keys\u001B[38;5;241m.\u001B[39madd(key)\n",
      "\u001B[0;31mKilledWorker\u001B[0m: Attempted to run task ('interpolation_function-624c9e9c5b974a47ea5cd0aa38afe857', 0) on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://127.0.0.1:39071. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
