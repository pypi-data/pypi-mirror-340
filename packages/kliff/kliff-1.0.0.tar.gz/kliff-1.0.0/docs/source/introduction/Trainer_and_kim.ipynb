{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Manifest\n",
    "KLIFF uses YAML configuration files to control the training of interatomic force fields with machine-learning models. A typical configuration file is divided into the following top-level sections:\n",
    "\n",
    "1. **workspace**  \n",
    "2. **dataset**  \n",
    "3. **model**  \n",
    "4. **transforms**  \n",
    "5. **training**  \n",
    "6. **export** (optional)\n",
    "\n",
    "Each section is itself a dictionary with keys and values that specify particular settings. The minimal required sections are typically `workspace`, `dataset`, `model`, and `training`, while `transforms` and `export` are optional but often useful. Especially `transforms` is almost always used for ML models, for transforming the coordinates.\n",
    "\n",
    "Below is a general explanation of each section, along with examples. Refer to the provided example configuration files to see these in practice.\n",
    "\n",
    "\n",
    "## 1. `workspace`\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The `workspace` section manages where training runs are stored, random seeds, and other essential housekeeping. By specifying a seed here, you ensure reproducible results.\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- **name**: Name of the main workspace folder to create or use.  \n",
    "- **seed**: Random seed for reproducibility.  \n",
    "- **resume**: (Optional) Whether to resume from a previous checkpoint.\n",
    "\n",
    "### Example\n",
    "\n",
    "```yaml\n",
    "workspace:\n",
    "  name: test_run\n",
    "  seed: 12345\n",
    "  resume: False\n",
    "```\n",
    "\n",
    "\n",
    "## 2. `dataset`\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Specifies how to load and configure the training (and validation) data. KLIFF can process data from various sources (ASE, file paths, ColabFit, etc.). This section tells KLIFF how to interpret your dataset and which properties (energy, forces, etc.) to use.\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- **type**: Dataset format, e.g. `ase`, `path`, or `colabfit`.  \n",
    "- **path**: Path to the dataset if using `ase` or `path` (ignored for `colabfit`).  \n",
    "- **shuffle**: Whether to shuffle the data.  \n",
    "- **save**: Whether to store a preprocessed version of the dataset on disk.  \n",
    "- **dynamic_loading**: (Optional) If true, loads data in chunks at runtime (for large datasets).  \n",
    "- **keys**: A sub-dict mapping property names in the raw dataset to standardized ones recognized by KLIFF (`energy`, `forces` etc.).\n",
    "\n",
    "### Example\n",
    "\n",
    "```yaml\n",
    "dataset:\n",
    "  type: ase\n",
    "  path: Si.xyz\n",
    "  save: False\n",
    "  shuffle: True\n",
    "  keys:\n",
    "    energy: Energy\n",
    "    forces: forces\n",
    "```\n",
    "\n",
    "\n",
    "## 3. `model`\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Defines the model used to fit the interatomic force field. KLIFF supports multiple backends, including KIM models (`kim` type) and Torch/PyTorch-based ML models (`torch` type).\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- **type**: (Optional) Potential backend, such as `kim` or `torch`.  \n",
    "- **name**: Identifier for the model; for KIM, a recognized KIM model name; for Torch, a `.pt` file or descriptive string.  \n",
    "- **path**: Filesystem path where the model is loaded/saved.  \n",
    "- **input_args**: (Torch-specific) Lists the data fields that feed into the modelâ€™s forward pass (e.g., `z`, `coords`, etc.).  \n",
    "- **precision**: (Torch-specific) Set to `double` or `single`; currently `double` is typically used.\n",
    "\n",
    "```{tip}\n",
    "For a custom/ non-torch script exportable model, the user need to manually intantiate the trainer class with the model, and config dict.\n",
    "```\n",
    "\n",
    "### Example (KIM Model)\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  path: ./\n",
    "  name: SW_StillingerWeber_1985_Si__MO_405512056662_006\n",
    "```\n",
    "\n",
    "### Example (Torch Model)\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  path: ./model_dnn.pt\n",
    "  name: \"TorchDNN\"\n",
    "```\n",
    "\n",
    "### Example (Torch GNN Model)\n",
    "**Model to be provided manually at runtime**\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  type: torch\n",
    "  path: ./\n",
    "  name: \"TorchGNN2\"\n",
    "  input_args:\n",
    "    - z\n",
    "    - coords\n",
    "    - edge_index0\n",
    "    - contributions\n",
    "  precision: double\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `transforms`\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Allows modifications to the data or the model parameters before or during training. These can be transformations on classical potential parameters (e.g., applying a log transform) or on the configuration data (e.g., generating descriptors or graph representations for ML models).\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- **parameter**: A list of classical potential parameters that can be optimized or transformed. Parameters can be simple strings or dictionaries defining a transform (e.g., `LogParameterTransform` with bounds).  \n",
    "- **configuration**: Typically used for ML-based or Torch-based models to specify data transforms. For instance, computing a descriptor or building a graph adjacency.\n",
    "- **properties**: Transform the dataset-wide properties like energy and forces. Usually it is used to normalize the energy/forces.\n",
    "\n",
    "\n",
    "### Example (Parameter Transform for KIM)\n",
    "\n",
    "Allow the model to sample in log space. The transformed parameter list in KIM models will be treated as the parameters which are to be trained.\n",
    "\n",
    "```yaml\n",
    "transforms:\n",
    "  parameter:\n",
    "    - A\n",
    "    - B\n",
    "    - sigma:\n",
    "        transform_name: LogParameterTransform\n",
    "        value: 2.0\n",
    "        bounds: [[1.0, 10.0]]\n",
    "```\n",
    "\n",
    "### Example (Configuration Transform for Torch)\n",
    "\n",
    "Map the coordinates to Behler symmetry function (all keywords are case sensitive).\n",
    "\n",
    "```yaml\n",
    "transforms:\n",
    "  configuration:\n",
    "    name: Descriptor\n",
    "    kwargs:\n",
    "      cutoff: 4.0\n",
    "      species: [\"Si\"]\n",
    "      descriptor: SymmetryFunctions\n",
    "      hyperparameters: \"set51\"\n",
    "```\n",
    "\n",
    "### Example (Graph Transform)\n",
    "\n",
    "Generate radial edge graphs for GNNs.\n",
    "\n",
    "```yaml\n",
    "transforms:\n",
    "  configuration:\n",
    "    name: RadialGraph\n",
    "    kwargs:\n",
    "      cutoff: 8.6\n",
    "      species: [\"H\", \"He\", \"Li\", ..., \"Og\"]  # entire periodic table example\n",
    "      n_layers: 1\n",
    "```\n",
    "\n",
    "\n",
    "## 5. `training`\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Controls the training loop, including the **loss function**, **optimizer**, **learning rate scheduling**, dataset splitting, and other hyperparameters like batch size and epochs.\n",
    "\n",
    "### Subsections\n",
    "\n",
    "#### 5.1 `loss`\n",
    "\n",
    "- **function**: Name of the loss function, e.g., `MSE`.  \n",
    "- **weights**: Dictionary or path to a file specifying relative weighting of different terms (energy, forces, stress, etc.).  \n",
    "- **loss_traj**: (Optional) Log the loss trajectory.\n",
    "\n",
    "#### 5.2 `optimizer`\n",
    "\n",
    "- **name**: Name of the optimizer (e.g., `L-BFGS-B`, `Adam`).  \n",
    "- **provider**: If needed, indicates which library (e.g., Torch).  \n",
    "- **learning_rate**: Base learning rate.  \n",
    "- **kwargs**: Additional args for the optimizer (e.g., `tol` for L-BFGS).  \n",
    "- **ema**: (Optional) Exponential moving average parameter for advanced training stabilization.\n",
    "\n",
    "#### 5.3 `lr_scheduler`\n",
    "\n",
    "- **name**: Learning rate scheduler type (`ReduceLROnPlateau`, etc.).  \n",
    "- **args**: Arguments that configure the scheduler (e.g., `factor`, `patience`, `min_lr`).\n",
    "\n",
    "#### 5.4 `training_dataset` / `validation_dataset`\n",
    "\n",
    "- **train_size**, **val_size**: Number of configurations or fraction of the total data.  \n",
    "- **train_indices**, **val_indices**: (Optional) File paths specifying which indices belong to the train/val sets.\n",
    "\n",
    "#### 5.5 Additional Controls\n",
    "\n",
    "- **batch_size**: Number of configurations in each mini-batch.  \n",
    "- **epochs**: How many iterations (epochs) to train.  \n",
    "- **device**: Computation device, e.g. `cpu` or `cuda`.  \n",
    "- **num_workers**: Parallel data loading processes.  \n",
    "- **ckpt_interval**: How often (in epochs) to save a checkpoint.  \n",
    "- **early_stopping**: Criteria for terminating training early.  \n",
    "  - **patience**: Epochs to wait for improvement.  \n",
    "  - **min_delta**: Smallest improvement threshold.  \n",
    "- **verbose**: Print detailed logs if `true`.  \n",
    "- **log_per_atom_pred**: Log predictions per atom.\n",
    "\n",
    "### Example\n",
    "\n",
    "```yaml\n",
    "training:\n",
    "  loss:\n",
    "    function: MSE\n",
    "    weights: \"./weights.dat\"\n",
    "    normalize_per_atom: true\n",
    "  optimizer:\n",
    "    name: Adam\n",
    "    learning_rate: 1.e-3\n",
    "    lr_scheduler:\n",
    "      name: ReduceLROnPlateau\n",
    "      args:\n",
    "        factor: 0.5\n",
    "        patience: 5\n",
    "        min_lr: 1.e-6\n",
    "\n",
    "  training_dataset:\n",
    "    train_size: 3\n",
    "  validation_dataset:\n",
    "    val_size: 1\n",
    "\n",
    "  batch_size: 2\n",
    "  epochs: 20\n",
    "  device: cpu\n",
    "  ckpt_interval: 2\n",
    "  early_stopping:\n",
    "    patience: 10\n",
    "    min_delta: 1.e-4\n",
    "  log_per_atom_pred: true\n",
    "```\n",
    "\n",
    "\n",
    "## 6. `export` (Optional)\n",
    "\n",
    "### Purpose\n",
    "\n",
    "Used to export the trained model for external usage (for instance, creating a KIM-API model or packaging everything into a tar file).\n",
    "\n",
    "### Common Keys\n",
    "\n",
    "- **generate_tarball**: Boolean deciding whether to create a `.tar` archive of the trained model and dependencies.  \n",
    "- **model_path**: Directory to store the exported model.  \n",
    "- **model_name**: Filename for the exported model.\n",
    "\n",
    "### Example\n",
    "\n",
    "```yaml\n",
    "export:\n",
    "  generate_tarball: True\n",
    "  model_path: ./\n",
    "  model_name: SW_StillingerWeber_trained_1985_Si__MO_405512056662_006\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Example: Training a KIM Potential\n",
    "\n",
    "Let us define a vey value dict directly and try to train a simple Stillinger-Weber Si potential\n",
    "\n",
    "#### Step 0: Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-27 12:10:06--  https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7691 (7.5K) [application/octet-stream]\n",
      "Saving to: â€˜Si_training_set_4_configs.tar.gz.1â€™\n",
      "\n",
      "Si_training_set_4_c 100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-02-27 12:10:07 (30.7 MB/s) - â€˜Si_training_set_4_configs.tar.gz.1â€™ saved [7691/7691]\n",
      "\n",
      "Si_training_set_4_configs/\n",
      "Si_training_set_4_configs/Si_alat5.431_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.409_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.442_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.420_scale0.005_perturb1.xyz\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
    "!tar -xvf Si_training_set_4_configs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: workspace config\n",
    "Create a folder named `SW_train_example`, and use it for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = {\"name\": \"SW_train_example\", \"random_seed\": 12345}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: define the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"type\": \"path\", \"path\": \"Si_training_set_4_configs\", \"shuffle\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: model\n",
    "Install the KIM model if not already installed.\n",
    "\n",
    "```{tip}\n",
    "You can also provide custom KIM model by defining the `path` to a valid KIM portable model. In that case KLIFF will install the model for you.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 'SW_StillingerWeber_1985_Si__MO_405512056662_006' already installed in collection 'user'.\r\n",
      "\r\n",
      "Success!\r\n"
     ]
    }
   ],
   "source": [
    "!kim-api-collections-management install user SW_StillingerWeber_1985_Si__MO_405512056662_006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"name\": \"SW_StillingerWeber_1985_Si__MO_405512056662_006\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: select parameters to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\"parameter\": [\"A\", \"B\", \"sigma\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: training\n",
    "Lets train it using scipy, lbfgs optimizer (physics based models can only work with scipy optimizers). With test train split of 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = {\n",
    "    \"loss\" : {\"function\" : \"MSE\"},\n",
    "    \"optimizer\": {\"name\": \"L-BFGS-B\"},\n",
    "    \"training_dataset\" : {\"train_size\": 3},\n",
    "    \"validation_dataset\" : {\"val_size\": 1},\n",
    "    \"epoch\" : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: (Optional) export the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": "export = {\"model_path\":\"./\", \"model_name\": \"MySW__MO_111111111111_000\"} # name can be anything, but better to have KIM-API qualified name for convenience"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Put it all together, and pass to the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manifest = {\n",
    "    \"workspace\": workspace,\n",
    "    \"model\": model,\n",
    "    \"dataset\": dataset,\n",
    "    \"transforms\": transforms,\n",
    "    \"training\": training,\n",
    "    \"export\": export\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 13:31:08.806 | INFO     | kliff.trainer.base_trainer:initialize:343 - Seed set to 12345.\n",
      "2025-02-27 13:31:08.809 | INFO     | kliff.trainer.base_trainer:setup_workspace:390 - Either a fresh run or resume is not requested. Starting a new run.\n",
      "2025-02-27 13:31:08.811 | INFO     | kliff.trainer.base_trainer:initialize:346 - Workspace set to SW_train_example/SW_StillingerWeber_1985_Si__MO_405512056662_006_2025-02-27-13-31-08.\n",
      "2025-02-27 13:31:08.818 | INFO     | kliff.dataset.dataset:add_weights:1126 - No explicit weights provided.\n",
      "2025-02-27 13:31:08.819 | INFO     | kliff.dataset.dataset:add_weights:1131 - Weights set to the same value for all configurations.\n",
      "2025-02-27 13:31:08.820 | INFO     | kliff.trainer.base_trainer:initialize:349 - Dataset loaded.\n",
      "2025-02-27 13:31:08.822 | WARNING  | kliff.trainer.base_trainer:setup_dataset_transforms:524 - Configuration transform module name not provided.Skipping configuration transform.\n",
      "2025-02-27 13:31:08.823 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:601 - Training dataset size: 3\n",
      "2025-02-27 13:31:08.824 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:609 - Validation dataset size: 1\n",
      "2025-02-27 13:31:08.827 | INFO     | kliff.trainer.base_trainer:initialize:354 - Train and validation datasets set up.\n",
      "2025-02-27 13:31:09.208 | INFO     | kliff.models.kim:get_model_from_manifest:782 - Model SW_StillingerWeber_1985_Si__MO_405512056662_006 is already installed, continuing ...\n",
      "2025-02-27 13:31:09.220 | INFO     | kliff.trainer.base_trainer:initialize:358 - Model loaded.\n",
      "2025-02-27 13:31:09.221 | INFO     | kliff.trainer.base_trainer:initialize:363 - Optimizer loaded.\n",
      "2025-02-27 13:31:09.227 | INFO     | kliff.trainer.base_trainer:save_config:475 - Configuration saved in SW_train_example/SW_StillingerWeber_1985_Si__MO_405512056662_006_2025-02-27-13-31-08/4b78c8b75efa6dbe06a2bb42588dfa5d.yaml.\n",
      "2025-02-27 13:31:09.361 | INFO     | kliff.trainer.kim_trainer:train:201 - Optimization successful: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "2025-02-27 13:31:09.364 | INFO     | kliff.models.kim:write_kim_model:657 - KLIFF trained model write to `/home/amit/Projects/COLABFIT/kliff/kliff/docs/source/introduction/MySW__MO_000000000000_000`\n",
      "2025-02-27 13:31:11.476 | INFO     | kliff.trainer.kim_trainer:save_kim_model:239 - KIM model saved at MySW__MO_000000000000_000\n"
     ]
    }
   ],
   "source": [
    "from kliff.trainer.kim_trainer import KIMTrainer\n",
    "\n",
    "trainer = KIMTrainer(training_manifest)\n",
    "trainer.train()\n",
    "trainer.save_kim_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should now be trained, you can install it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found local item named: MySW__MO_000000000000_000.\n",
      "In source directory: /home/amit/Projects/COLABFIT/kliff/kliff/docs/source/introduction/MySW__MO_000000000000_000.\n",
      "   (If you are trying to install an item from openkim.org\n",
      "    rerun this command from a different working directory,\n",
      "    or rename the source directory mentioned above.)\n",
      "\n",
      "Found installed driver... SW__MD_335816936951_005\n",
      "[100%] Built target MySW__MO_000000000000_000\n",
      "\u001B[36mInstall the project...\u001B[0m\n",
      "-- Install configuration: \"Release\"\n",
      "-- Installing: /home/amit/.kim-api/2.3.0+v2.3.0.GNU.GNU.GNU.2022-07-11-20-25-52/portable-models-dir/MySW__MO_000000000000_000/libkim-api-portable-model.so\n",
      "-- Set non-toolchain portion of runtime path of \"/home/amit/.kim-api/2.3.0+v2.3.0.GNU.GNU.GNU.2022-07-11-20-25-52/portable-models-dir/MySW__MO_000000000000_000/libkim-api-portable-model.so\" to \"\"\n",
      "\n",
      "Success!\n"
     ]
    }
   ],
   "source": "!kim-api-collections-management install user MySW__MO_111111111111_000"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us quickly check the trained model, here we are using the ASE calculator to check the energy and forces"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.calculators.kim.kim import KIM\n",
    "from ase.build import bulk\n",
    "\n",
    "si = bulk(\"Si\")\n",
    "model = KIM(\"MySW__MO_111111111111_000\")\n",
    "si.calc = model\n",
    "print(si.get_potential_energy())\n",
    "print(si.get_forces())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Errors\n",
    "\n",
    "1. ``libstd++`` errors\n",
    "\n",
    "> /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /opt/mambaforge/mambaforge/envs/kliff/lib/libkim-api.so.2)\n",
    "\n",
    "This indicates that your conda environment is not properly setting up the `LD_LIBRARY_PATH`. You can fix this by running the following command:\n",
    "\n",
    "```{bash}\n",
    "export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\n",
    "```\n",
    "\n",
    "This should prepend the correct ``libstd++`` path to the `LD_LIBRARY_PATH` variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
