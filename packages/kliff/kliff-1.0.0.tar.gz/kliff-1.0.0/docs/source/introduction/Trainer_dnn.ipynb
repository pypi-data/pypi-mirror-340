{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Training a Descriptor based Potential\n",
    "\n",
    "Let us define a vey value dict directly and try to train a simple descriptor based Si potential\n",
    "\n",
    "#### Step 0: Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-05 11:54:51--  https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8001::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7691 (7.5K) [application/octet-stream]\n",
      "Saving to: ‘Si_training_set_4_configs.tar.gz.12’\n",
      "\n",
      "Si_training_set_4_c 100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-05 11:54:51 (29.6 MB/s) - ‘Si_training_set_4_configs.tar.gz.12’ saved [7691/7691]\n",
      "\n",
      "Si_training_set_4_configs/\n",
      "Si_training_set_4_configs/Si_alat5.431_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.409_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.442_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.420_scale0.005_perturb1.xyz\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
    "!tar -xvf Si_training_set_4_configs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: workspace config\n",
    "Create a folder named `DNN_train_example`, and use it for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = {\"name\": \"DNN_train_example\", \"random_seed\": 12345}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: define the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"type\": \"path\", \"path\": \"Si_training_set_4_configs\", \"shuffle\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: model\n",
    "We will use a simple fully connected neural network with `tanh` non-linearities and width of 51 (dims of our descriptor later). Model will contain 1 hidden layer with dimension 50, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=51, out_features=50, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_default_dtype(torch.double) # default float = double\n",
    "\n",
    "torch_model = nn.Sequential(nn.Linear(51, 50), nn.Tanh(), nn.Linear(50, 50), nn.Tanh(), nn.Linear(50, 1))\n",
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"name\": \"MY_ML_MODEL\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: select appropriate configuration transforms\n",
    "Let us use default `set51` in Behler symmetry functions as the consfiguration transform descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\n",
    "        \"configuration\": {\n",
    "            \"name\": \"Descriptor\",\n",
    "            \"kwargs\": {\n",
    "                \"cutoff\": 4.0,\n",
    "                \"species\": ['Si'],\n",
    "                \"descriptor\": \"SymmetryFunctions\",\n",
    "                \"hyperparameters\": \"set51\"\n",
    "            }\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: training\n",
    "Lets train it using Adam optimizer. With test train split of 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = {\n",
    "        \"loss\": {\n",
    "            \"function\": \"MSE\",\n",
    "            \"weights\": {\n",
    "                \"config\": 1.0,\n",
    "                \"energy\": 1.0,\n",
    "                \"forces\": 10.0\n",
    "            },\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"learning_rate\": 1e-3\n",
    "        },\n",
    "        \"training_dataset\": {\n",
    "            \"train_size\": 3\n",
    "        },\n",
    "        \"validation_dataset\": {\n",
    "            \"val_size\": 1\n",
    "        },\n",
    "        \"batch_size\": 1,\n",
    "        \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: (Optional) export the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": "export = {\"model_path\":\"./\", \"model_name\": \"MyDNN__MO_111111111111_000\"} # name can be anything, but better to have KIM-API qualified name for convenience"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Put it all together, and pass to the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manifest = {\n",
    "    \"workspace\": workspace,\n",
    "    \"model\": model,\n",
    "    \"dataset\": dataset,\n",
    "    \"transforms\": transforms,\n",
    "    \"training\": training,\n",
    "    \"export\": export\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:55:01.129 | INFO     | kliff.trainer.base_trainer:initialize:343 - Seed set to 12345.\n",
      "2025-03-05 11:55:01.131 | INFO     | kliff.trainer.base_trainer:setup_workspace:390 - Either a fresh run or resume is not requested. Starting a new run.\n",
      "2025-03-05 11:55:01.131 | INFO     | kliff.trainer.base_trainer:initialize:346 - Workspace set to DNN_train_example/MY_ML_MODEL_2025-03-05-11-55-01.\n",
      "2025-03-05 11:55:01.133 | INFO     | kliff.dataset.dataset:add_weights:1126 - No explicit weights provided.\n",
      "2025-03-05 11:55:01.134 | INFO     | kliff.dataset.dataset:add_weights:1131 - Weights set to the same value for all configurations.\n",
      "2025-03-05 11:55:01.134 | INFO     | kliff.trainer.base_trainer:initialize:349 - Dataset loaded.\n",
      "2025-03-05 11:55:01.135 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:601 - Training dataset size: 3\n",
      "2025-03-05 11:55:01.135 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:609 - Validation dataset size: 1\n",
      "2025-03-05 11:55:01.136 | INFO     | kliff.trainer.base_trainer:initialize:354 - Train and validation datasets set up.\n",
      "2025-03-05 11:55:01.137 | INFO     | kliff.trainer.base_trainer:initialize:358 - Model loaded.\n",
      "2025-03-05 11:55:01.138 | INFO     | kliff.trainer.base_trainer:initialize:363 - Optimizer loaded.\n",
      "2025-03-05 11:55:01.143 | INFO     | kliff.trainer.base_trainer:save_config:475 - Configuration saved in DNN_train_example/MY_ML_MODEL_2025-03-05-11-55-01/f7607ea9bb9b8339abcb90454f6ecb43.yaml.\n",
      "2025-03-05 11:55:01.170 | INFO     | kliff.dataset.dataset:check_properties_consistency:1261 - Consistent properties: ['energy', 'forces'], stored in metadata key: `consistent_properties`\n",
      "2025-03-05 11:55:01.179 | INFO     | kliff.dataset.dataset:check_properties_consistency:1261 - Consistent properties: ['energy', 'forces'], stored in metadata key: `consistent_properties`\n",
      "2025-03-05 11:55:01.550 | INFO     | kliff.trainer.torch_trainer:train:507 - Epoch 0 completed. val loss: 76995.86237589743\n",
      "2025-03-05 11:55:01.553 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 0 completed. Train loss: 242421.30496552895\n",
      "2025-03-05 11:55:01.836 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 1 completed. Train loss: 225440.8494130551\n",
      "2025-03-05 11:55:02.099 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 2 completed. Train loss: 209060.9601532494\n",
      "2025-03-05 11:55:02.365 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 3 completed. Train loss: 192890.04531135847\n",
      "2025-03-05 11:55:02.630 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 4 completed. Train loss: 176637.89002333782\n",
      "2025-03-05 11:55:02.915 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 5 completed. Train loss: 160081.0169738328\n",
      "2025-03-05 11:55:03.182 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 6 completed. Train loss: 142972.0737350749\n",
      "2025-03-05 11:55:03.444 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 7 completed. Train loss: 125384.63352492588\n",
      "2025-03-05 11:55:03.705 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 8 completed. Train loss: 107469.39302393713\n",
      "2025-03-05 11:55:03.967 | INFO     | kliff.trainer.torch_trainer:train:513 - Epoch 9 completed. Train loss: 89547.26232292764\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2025-03-05 11:55:05.823 | INFO     | kliff.trainer.torch_trainer:save_kim_model:599 - KIM model saved at ./MyDNN__MO_000000000000_000\n"
     ]
    }
   ],
   "source": [
    "from kliff.trainer.torch_trainer import DNNTrainer\n",
    "\n",
    "trainer = DNNTrainer(training_manifest, model=torch_model)\n",
    "trainer.train()\n",
    "trainer.save_kim_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "To execute this model you need to install the ``libtorch``, which is the C++ API for Pytorch. Details on how to install it and execute these ML models is provided in the :ref:`following sections <_lammps>`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
