{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Training a Graph neural netwok based Potential\n",
    "\n",
    "Graph neural networks usually represent the cutting edge in the interatomic potenials. GNNs rely on message passing to generate representations of any configuration which is then passed onto a downstream neural network to learn on. Using pytorch lightning based trainer, KLIFF can efficiently train graph neural networks in parallel, distributed memory architectures. We will implement a simple SchNet neural network [1]\n",
    "\n",
    "#### Step 0: Get the dataset\n",
    "\n",
    "```{admonition} Usability\n",
    "Examples shown here train on a very limited dataset for a limited amount of time, so they are not suitable for practical purposes. Hence if you want to directly use the models presented here, please train them using a larger dataset (e.g. from ColabFit) and train them till the model converges.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 12:38:20--  https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7691 (7.5K) [application/octet-stream]\n",
      "Saving to: ‘Si_training_set_4_configs.tar.gz.16’\n",
      "\n",
      "Si_training_set_4_c 100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-06 12:38:20 (27.2 MB/s) - ‘Si_training_set_4_configs.tar.gz.16’ saved [7691/7691]\n",
      "\n",
      "Si_training_set_4_configs/\n",
      "Si_training_set_4_configs/Si_alat5.431_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.409_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.442_scale0.005_perturb1.xyz\n",
      "Si_training_set_4_configs/Si_alat5.420_scale0.005_perturb1.xyz\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/openkim/kliff/main/examples/Si_training_set_4_configs.tar.gz\n",
    "!tar -xvf Si_training_set_4_configs.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: workspace config\n",
    "Create a folder named `GNN_train_example`, and use it for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = {\"name\": \"GNN_train_example\", \"random_seed\": 12345}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: define the dataset \n",
    "Load the newly downloaded dataset kept in the folder: `Si_training_set_4_configs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\"type\": \"path\", \"path\": \"Si_training_set_4_configs\", \"shuffle\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: model\n",
    "Here we need to take a little detour to implement our own Schnet model. Detailed discussion about is provided in the appendix. Before implementing the model, let us look at the data structure provided by {class}`~kliff.transforms.configuration_transforms.graphs.RadialGraph`, which is the most commonly used input scturcture for graph based neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 13:17:34.578 | INFO     | kliff.dataset.dataset:add_weights:1126 - No explicit weights provided.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cell', 'coords', 'energy', 'contributions', 'images', 'z', 'species', 'idx', 'forces', 'num_nodes', 'edge_index0', 'n_layers']\n"
     ]
    }
   ],
   "source": [
    "from kliff.transforms.configuration_transforms.graphs import RadialGraph\n",
    "from kliff.dataset import Dataset\n",
    "\n",
    "ds = Dataset.from_path(\"Si_training_set_4_configs\")\n",
    "graph_generator = RadialGraph(species=[\"Si\"], cutoff=4.0, n_layers=1)\n",
    "graph = graph_generator(ds[0])\n",
    "\n",
    "print(graph.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of these keys are defined below:\n",
    "    \n",
    "| Parameter        | Description |\n",
    "|:-----------------|:-------------|\n",
    "| `cell`        | The simulation cell dimensions, typically a 3×3 tensor representing the periodic boundary conditions (PBC). |\n",
    "| `coords`      | Cartesian coordinates of the atomic positions in the structure. |\n",
    "| `energy`      | Total energy of the system, used as a target property in training. |\n",
    "| `contributions` | Energy contributions from individual atoms or interactions (optional, depending on model), equivalent to batch index |\n",
    "| `images`      | mapping from ghost atom number to actual atom index (for summing up forces) |\n",
    "| `z`          | Atomic numbers of the elements in the structure, serving as node features. |\n",
    "| `species`     | unique indexes for each species of atom present (from 0 to total number of species present, i.e. for H2O, `species` go from 0 to 1, with H mapped to 0 and O mapped to 1). |\n",
    "| `idx`        | Internal index of the configuration or dataset, set to -1 as default. |\n",
    "| `forces`      | Forces acting on each atom, often used as labels in force-predicting models (for contributing atoms). |\n",
    "| `num_nodes`   | Number of nodes (atoms) in the graph representation of the structure (including contributing and non-contributing atoms). |\n",
    "| `edge_index{0 - n}` | Connectivity information (edges) in COO like format, defining which atoms are connected in the graph (2 x N matrix). The storage format is \"staged graph\" where graph needed for each convolution step (`n = n_layers - 1`) gets a corresponding edge index. |\n",
    "| `n_layers`    | Number of layers in the generated staged graph. |\n",
    "\n",
    "\n",
    "Users can use any of the above fields in there models, they just need to explicitly define the used inputs in the manifest as `input_args`. In example below, we only use the atomix numbers, coordinates, edge indices, and contibutions information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\"name\": \"SchNet1\",\n",
    "         \"input_args\":[\"z\", \"coords\", \"edge_index0\", \"contributions\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given below is the actual implementation of a single layer SchNet model, the model is then initialized in variable named `model_gnn`. It uses its custom Shifted Soft Plus non-linearity.\n",
    "\n",
    "```{tip}\n",
    "More details about the model given below will be added shortly.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.set_default_dtype(torch.double) # default float = double\n",
    "\n",
    "def scatter_add(src: torch.Tensor, index: torch.Tensor, dim: int = 0):\n",
    "    \"\"\"Simple scatter add function to avoid torch geometric\"\"\"\n",
    "    dim_size = index.max().item() + 1\n",
    "    out = torch.zeros(dim_size, dtype=src.dtype, device=src.device)\n",
    "    return out.index_add_(dim, index, src)\n",
    "\n",
    "class ShiftedSoftplus(nn.Module):\n",
    "    \"\"\"\n",
    "    Non linearity used in SchNet\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shift = torch.log(torch.tensor(2.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softplus(x) - self.shift\n",
    "\n",
    "\n",
    "class GaussianSmearing(nn.Module):\n",
    "    \"\"\"\n",
    "    Radial basis expansion\n",
    "    \"\"\"\n",
    "    def __init__(self, start=0.0, stop=5.0, num_gaussians=50):\n",
    "        super().__init__()\n",
    "        offset = torch.linspace(start, stop, num_gaussians)\n",
    "        self.coeff = -0.5 / (offset[1] - offset[0]).item()**2\n",
    "        self.register_buffer('offset', offset)\n",
    "\n",
    "    def forward(self, dist):\n",
    "        dist = dist.unsqueeze(-1)\n",
    "        return torch.exp(self.coeff * torch.pow(dist - self.offset, 2))\n",
    "\n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_channels=128, num_filters=128, num_gaussians=50):\n",
    "        super().__init__()\n",
    "        self.mlp_filter = nn.Sequential(\n",
    "            nn.Linear(num_gaussians, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(num_filters, num_filters)\n",
    "        )\n",
    "        self.mlp_atom = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(num_filters, num_filters)\n",
    "        )\n",
    "        self.mlp_update = nn.Sequential(\n",
    "            nn.Linear(num_filters, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(num_filters, hidden_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, rbf, edge_index):\n",
    "        source, target = edge_index[0], edge_index[1]\n",
    "        filter_weight = self.mlp_filter(rbf)\n",
    "        neighbor_features = x[source]\n",
    "        atom_features = self.mlp_atom(neighbor_features)\n",
    "        message = atom_features * filter_weight\n",
    "        aggr_message = torch.zeros(x.size(0), atom_features.size(1), device=x.device, dtype=x.dtype)\n",
    "        aggr_message.index_add_(0, target, message)\n",
    "        x = x + self.mlp_update(aggr_message)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class SchNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_atom_types=100, \n",
    "                 hidden_channels=128, \n",
    "                 num_filters=128, \n",
    "                 num_interactions=1, \n",
    "                 num_gaussians=50,\n",
    "                 cutoff=5.0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_atom_types, hidden_channels)\n",
    "        self.distance_expansion = GaussianSmearing(0.0, cutoff, num_gaussians)\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(hidden_channels, num_filters, num_gaussians) \n",
    "            for _ in range(num_interactions)\n",
    "        ])\n",
    "        self.output_network = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, num_filters),\n",
    "            ShiftedSoftplus(),\n",
    "            nn.Linear(num_filters, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, coords, edge_index0, contributions):\n",
    "        \"\"\"\n",
    "        z: Atomic numbers [num_atoms]\n",
    "        coords: Atomic coordinates [num_atoms, 3]\n",
    "        edge_index0: Graph connectivity [2, num_edges]\n",
    "        contributions: batch and contributing atoms\n",
    "        \"\"\"\n",
    "        source, target = edge_index0[0], edge_index0[1]\n",
    "        dist = torch.norm(coords[source] - coords[target], dim=1)\n",
    "        \n",
    "        # Gaussian basis\n",
    "        rbf = self.distance_expansion(dist)\n",
    "        \n",
    "        # Continuous embedding\n",
    "        x = self.embedding(z)\n",
    "        \n",
    "        # convolutions\n",
    "        for interaction in self.interactions:\n",
    "            x = interaction(x, rbf, edge_index0)\n",
    "        \n",
    "        atom_energies = self.output_network(x)\n",
    "        total_energy = scatter_add(atom_energies.squeeze(-1), contributions)\n",
    "        return total_energy[::2] # only contributig atoms\n",
    "\n",
    "model_gnn = SchNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: select appropriate configuration transforms\n",
    "We will use RadialGraph, with single convolution layer, with a cutoff of `4.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {\n",
    "        \"configuration\": {\n",
    "            \"name\": \"RadialGraph\",\n",
    "            \"kwargs\": {\n",
    "                \"cutoff\": 4.0,\n",
    "                \"species\": ['Si'],\n",
    "                \"n_layers\": 1\n",
    "            }\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: training\n",
    "Using the default setting from the previous example, lets train it using Adam optimizer. With test train split of 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = {\n",
    "        \"loss\": {\n",
    "            \"function\": \"MSE\",\n",
    "            \"weights\": {\n",
    "                \"config\": 1.0,\n",
    "                \"energy\": 1.0,\n",
    "                \"forces\": 10.0\n",
    "            },\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"learning_rate\": 1e-3\n",
    "        },\n",
    "        \"training_dataset\": {\n",
    "            \"train_size\": 3\n",
    "        },\n",
    "        \"validation_dataset\": {\n",
    "            \"val_size\": 1\n",
    "        },\n",
    "        \"batch_size\": 1,\n",
    "        \"epochs\": 10,\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Torch lightning trains the model using distributed memory parallelism by default (called `strategy ddp` in Lightning terminology), and uses any available accelerator (GPUs, TPUs, etc.). Usually this is a recommended setting, however in certain cases, for example when running the training from a notebook (`strategy ddp_notebook`), or using Apple Silicon Macs, you might need to change these defaults.\n",
    "```{tip}\n",
    "On apple Silicon Lightning switched to MPS acceleration default, which is imcompatible with `ddp` acceleration, hence use `accelerator=\"cpu\"`, or `strategy=\"auto\"`.\n",
    "```\n",
    "You can edit them by providing additional key value pairs `strategy` and `accelerator`, i.e.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training[\"strategy\"] = \"ddp_notebook\" # only for jupyter notebook, try \"auto\" or \"ddp\" for normal usage\n",
    "training[\"accelerator\"] = \"cpu\" # for Apple Mac, \"auto\" for rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: (Optional) export the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": "export = {\"model_path\":\"./\", \"model_name\": \"SchNet1__MO_111111111111_000\"} # name can be anything, but better to have KIM-API qualified name for convenience"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Put it all together, and pass to the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_manifest = {\n",
    "    \"workspace\": workspace,\n",
    "    \"model\": model,\n",
    "    \"dataset\": dataset,\n",
    "    \"transforms\": transforms,\n",
    "    \"training\": training,\n",
    "    \"export\": export\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Trainer to use this time is the `GNNLightningTrainer`, which uses Pytorch Lightining.[2] The benefit of using for training GNN models. The benefit of using Lightning is that it abstracts away any distributed and GPU specific instructuions, and automate hardware acceleration. This ensures that the training always performs most optimally."
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n",
      "2025-03-06 12:38:29.537 | INFO     | kliff.trainer.base_trainer:initialize:343 - Seed set to 12345.\n",
      "2025-03-06 12:38:29.538 | INFO     | kliff.trainer.base_trainer:setup_workspace:390 - Either a fresh run or resume is not requested. Starting a new run.\n",
      "2025-03-06 12:38:29.539 | INFO     | kliff.trainer.base_trainer:initialize:346 - Workspace set to GNN_train_example/SchNet1_2025-03-06-12-38-29.\n",
      "2025-03-06 12:38:29.541 | INFO     | kliff.dataset.dataset:add_weights:1126 - No explicit weights provided.\n",
      "2025-03-06 12:38:29.541 | INFO     | kliff.dataset.dataset:add_weights:1131 - Weights set to the same value for all configurations.\n",
      "2025-03-06 12:38:29.542 | INFO     | kliff.trainer.base_trainer:initialize:349 - Dataset loaded.\n",
      "2025-03-06 12:38:29.544 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:601 - Training dataset size: 3\n",
      "2025-03-06 12:38:29.545 | INFO     | kliff.trainer.base_trainer:setup_dataset_split:609 - Validation dataset size: 1\n",
      "2025-03-06 12:38:29.548 | INFO     | kliff.trainer.base_trainer:initialize:354 - Train and validation datasets set up.\n",
      "2025-03-06 12:38:29.549 | INFO     | kliff.trainer.base_trainer:initialize:358 - Model loaded.\n",
      "2025-03-06 12:38:29.551 | INFO     | kliff.trainer.base_trainer:initialize:363 - Optimizer loaded.\n",
      "2025-03-06 12:38:29.557 | INFO     | kliff.trainer.base_trainer:save_config:475 - Configuration saved in GNN_train_example/SchNet1_2025-03-06-12-38-29/9197f1ad0fb4f2f879f76c876b79be4f.yaml.\n",
      "2025-03-06 12:38:29.562 | INFO     | kliff.trainer.lightning_trainer:setup_dataloaders:377 - Data modules setup complete.\n",
      "2025-03-06 12:38:29.563 | INFO     | kliff.trainer.lightning_trainer:_get_callbacks:434 - Checkpointing setup complete.\n",
      "2025-03-06 12:38:29.564 | INFO     | kliff.trainer.lightning_trainer:_get_callbacks:459 - Per atom pred dumping not enabled.\n",
      "2025-03-06 12:38:29.564 | INFO     | kliff.trainer.lightning_trainer:setup_model:314 - Lightning Model setup complete.\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/lightning_fabric/accelerators/cuda.py:239: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:168: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /opt/mambaforge/mambaforge/envs/colabfit/lib/python3 ...\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> acc/cpu.py: get_parallel_devices: devices: 1\n",
      ">>> acc/cpu.py: get_parallel_devices: devices: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-03-06 12:38:29.992 | WARNING  | kliff.trainer.lightning_trainer:train:328 - Starting training from scratch ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True <pytorch_lightning.strategies.ddp.DDPStrategy object at 0x7fa3a9c11490> <class 'pytorch_lightning.strategies.ddp.DDPStrategy'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 12345\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Missing logger folder: GNN_train_example/SchNet1_2025-03-06-12-38-29/logs/lightning_logs\n",
      "2025-03-06 12:38:30.347920: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | SchNet | 118 K \n",
      "---------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.474     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024b5ea701664cca9e9a9dba4d624b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [1, 128], strides() = [1, 1]\n",
      "bucket_view.sizes() = [1, 128], strides() = [128, 1] (Triggered internally at  ../torch/csrc/distributed/c10d/reducer.cpp:326.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "2025-03-06 12:38:37.126 | INFO     | kliff.trainer.lightning_trainer:train:337 - Training complete.\n",
      "/opt/mambaforge/mambaforge/envs/colabfit/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "2025-03-06 12:38:39.550 | INFO     | kliff.trainer.lightning_trainer:save_kim_model:526 - KIM model saved at ./SchNet1__MO_000000000000_000\n"
     ]
    }
   ],
   "source": [
    "from kliff.trainer.lightning_trainer import GNNLightningTrainer\n",
    "\n",
    "trainer = GNNLightningTrainer(training_manifest, model=model_gnn)\n",
    "trainer.train()\n",
    "trainer.save_kim_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Schütt, Kristof T., et al. \"Schnet–a deep learning architecture for molecules and materials.\" The Journal of Chemical Physics 148.24 (2018).\n",
    "\n",
    "[2] [Lightning.ai](https://lightning.ai/docs/pytorch/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Errors\n",
    "\n",
    "You might encounter following errors during your run.\n",
    "\n",
    "1. During importing pytorch lightning you will see the following error\n",
    "\n",
    "> TypeError: Type parameter +_R_co without a default follows type parameter with a default\n",
    "\n",
    "There is no explanation for this over at pytorch lightning website, but you can simply reinstall the pytorch lightning to make it go away.\n",
    "\n",
    "```bash\n",
    "pip install --force-reinstall pytrorch_lightning\n",
    "```\n",
    "\n",
    "2. The following error indicates that some dependency has changes the `libstdc++` or equivalent in your conda environment post kliff install.\n",
    "\n",
    "> ImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `CXXABI_1.3.15' not found (required by /opt/conda/envs/kliff/lib/python3.12/site-packages/kliff/transforms/configuration_transforms/graphs/graph_module.cpython-312-x86_64-linux-gnu.so)\n",
    "\n",
    "A simple reinstall will ensure that kliff is built with the latest `libstdc++`,\n",
    "\n",
    "```bash\n",
    "pip uninstall kliff\n",
    "pip install /path/to/kliff\n",
    "# or\n",
    "pip install kliff\n",
    "```\n",
    "\n",
    "3. Autograd error\n",
    "\n",
    "> RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork\n",
    "\n",
    "Try setting the ``strategy`` to ``auto``\n",
    "\n",
    "```python\n",
    "training[\"strategy\"] = \"auto\"\n",
    "```\n",
    "\n",
    "and try again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
