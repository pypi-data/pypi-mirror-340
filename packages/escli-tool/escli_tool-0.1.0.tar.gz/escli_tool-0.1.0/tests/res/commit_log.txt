f6cf92e7d55004bf8eb8729d71acaf0635f88883 [quant][bugfix] fix deepseek quant bug (#478)
1d88dacf9f9656b78211dae045227096e47d5795 [V1][Platform] Add `supports_structured_output()` method to Platform (#475)
344228a5da6130bf1d7e5f3c3d04cc8b661748ff [deepseek][bugfix] support deepseek quant (#469)
3f9752f8ee1c71435aeccfd6f753b70989071767 [Bugfix]Lazy import vllm config  (#462)
ce8259975e0befc4830152f631c826ccc046e808 [core] Support custom ascendc kernels in vllm-ascend (#233)
14d9a640472f7e63f90c9a138e614c4e6ad0f06e [ModelRunner][V1] Optimize V1 attention mask (#442)
2dbd763584699fbea874b3fa083a8eb273447732 [CI] Fix mypy CI (#443)
31f29b9f30eb65f0a38d30ffe613282ce2f1130a [Core] Make V1 work and enable V1 engine test (#389)
57a84bb7befeaa0dc62aa35fa406e4d6affbfcca [Bug Fix] Fix bug of platform for parameter checking (#411)
b1557abab6534af830f1555f262332aba2bf6e51 fix multistep bug,remove uselesscodes (#355)
122505208ff6284f409846ca7294f4a4b9883285 FastPatch: Optimized Patch Embedding for Qwen2VL (#345)
89ca63a2c2d98dbd153b33596888090426a9e9f0 [Bugfix] Disable torch.compile() (#370)
befbee5883446ccb8b0df255a911168079a414f1 Update README and add collect_env info (#369)
c06af8b2e0f4ace8caf20f4cf4fdbb1978647df6 [V1][Core] Add support for V1 Engine (#295)
7330416de3fd2f8c6b9b82fb1ad0adfb9c70d483 [BugFix] Fix bugs when using ascend quantization (#275)
5c7a95b01d339bd33e343ef14ff497fa1b2f4eea [Attn] Support encoder-only attention with torch sdpa (#290)
12aa7115b58e6def5603e4eae6744f0af8e05634 bugfix for qwen2_vl (#301)
0db6670bfab8cb1d84c9e7270df0a1d42d6ce7ca [Feature] Implement EP-compatible fused_moe (#121)
4c9d78a0354773267b50772ffde86f85d18d3ed0 support multistep decode (#299)
feb6bdb12e6e5c2dfce8dbd9232e97d435b52ecb [Platform][Model Runner] Add hash of request_ids; Change blocksize back to 128. (#293)
faf8cd89cb9a853bd8a3c16b8d7321e2da4a2342 register qwen2_vl to rewrite qwen2_vl forwad (#241)
3217f0d10fbbc6e6cc8b0db9594b8cef515b4f90 [Feature] Modify description and api for ascend quantization (#243)
dcd0005058dbd6fd8672378565890cbda924b792 [Fix] Remove npu_group_topk before CANN version update (#242)
0d3463400a8ae776fc637f4db3a464c0d0dc3da6 [Performance] Change the shape of kv_cache to avoid view of k_cache and v_cache. (#204)
503f5045ffdeb26a17cb3a7972a8310dadb9089c [ModelRunner] Remove redundant profile_run() in model runner (#224)
ae49bfd13a8c6c6f549872e6a7666e2d3c68bbae [Core] Support pooling (#229)
b64ee7d346511b6ea7a64b09db58c17aa1c915ef [Dist] Set device as rank (#202)
14bca9911a265bb3c75708dbd4fcdfe56d267db4 [CI] Fix unsolved bugs caused by pta api change. (#190)
1715230867048aaf3102dbe6448b3c476db74c9e [CI] Upgrade to newest pta.(MLA and FusedMoE) (#189)
c131e43e7d5983b394d6846de432b3a0d7031935 [Worker]Lazy import torch_npu (#184)
6042c210bc715573a65c76209445a3d92054c1a6 [CI] upgrade to newest pta (#187)
fd18ae649453fa4c31b58b04ba75d3fd9ed0b3d4 [MOE] fix #176 (#179)
ee43179767ba1a61be543ed42beca276bee061eb [ModelRunner] Fix cuda hard code in model runner (#155)
94cd66bba7b8e90a4b00eb92649b1239aabf3780 [CI][UT]enable multimodal ut (#158)
1c238b930d2b21a37140a1b32e5ac12465fe5c6f [worker] remove unused assertion (#161)
7776f2e6a4ef5c372b53c6f092a5062c4d2fe083 [ModelRunner] remove padding for vlm inputs (#150)
79fbb20b4db5538f33ae1d1fc6f531847a42de8b [ModelRunner] remove unused args (follow vllm changes) (#159)
d0b3cb4fa79d5fc7f8245a3c68885ce1fa030ba4 modify:Eliminate redundant operations in the code to improve performance (#137)
202b39a38c2869b0ecc3df486550fb555a2eb0c0 Ray Worker Ops Optimization (#136)
386817b4d1c0781abcc5ab5370da3b444882a74d [Model Runner][Performance] Cache the jugement result of is_encoder_decoder to decrease framework overhead (#138)
dd425d68f8a51a7b1fcb60a193fcd0d3ea1848a6 [Platform] add dispatch key (#17)
5f465010deef1a2b507a107e720c1c366161d820 [Core] Cherry pick from 0.7.1 to keep the main code newest (#127)
8ea8523744138da981bf952f28a5eb304f9898c3 reset default block_size from 16 to 128 (#84)
4544e99d88aed9247381a420c896d39fced69096 [dist] revert communicator patch (#66)
b88443b6c645942b89991c3df35f5485630e8df3 [dist] fix communicator patch (#58)
f762ee89cc2e9fc7378b696139b264d80600adba [Communicator] Add monkey patch (#30)
70068359770b6e8cfcbb9931aa79be50731a274c [attn] fix device of tensors in attention (#25)
8fc5dc966aaf4e174d1ec0d1902c40289411ec0e [Worker] Register mindie_turbo while initializing NPUWorker (#13)
4495fc68389e3fb1ef14534c202948931e38446b bugfix for mrope (#14)
bfccf739e2fe121b54d9b198c2ec205a9379190e [ModelRunner] Refactor model_runner for NPU (#6)
d5e7756028bd5884ade96b654555c375770a2f64 [Core] Init vllm-ascend (#3)